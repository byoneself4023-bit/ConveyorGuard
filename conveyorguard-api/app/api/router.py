"""
ConveyorGuard - API Router
"""

import numpy as np
import torch
from fastapi import APIRouter, HTTPException, status
import logging

from app.api.schemas import (
    PredictRequest, PredictResponse,
    BatchPredictRequest, BatchPredictResponse,
    HealthResponse, ModelInfoResponse, STATE_LABELS
)
from app.core.loader import model_loader
from app.core.preprocessing import preprocess_input

logger = logging.getLogger(__name__)
router = APIRouter()


# ============== ì…ë ¥ ê²€ì¦ í•¨ìˆ˜ ==============

def validate_input_shape(sensors, images, external):
    """ì…ë ¥ ë°ì´í„° shape ê²€ì¦"""
    errors = []
    
    # sensors: (30, 8)
    if len(sensors) != 30:
        errors.append(f"sensors: ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 30ì´ì–´ì•¼ í•©ë‹ˆë‹¤ (í˜„ì¬: {len(sensors)})")
    elif len(sensors[0]) != 8:
        errors.append(f"sensors: ê° íƒ€ì„ìŠ¤í…ì€ 8ê°œ ì„¼ì„œê°’ì´ì–´ì•¼ í•©ë‹ˆë‹¤ (í˜„ì¬: {len(sensors[0])})")
    
    # images: (30, 60, 80)
    if len(images) != 30:
        errors.append(f"images: ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 30ì´ì–´ì•¼ í•©ë‹ˆë‹¤ (í˜„ì¬: {len(images)})")
    elif len(images[0]) != 60:
        errors.append(f"images: ì´ë¯¸ì§€ ë†’ì´ê°€ 60ì´ì–´ì•¼ í•©ë‹ˆë‹¤ (í˜„ì¬: {len(images[0])})")
    elif len(images[0][0]) != 80:
        errors.append(f"images: ì´ë¯¸ì§€ ë„ˆë¹„ê°€ 80ì´ì–´ì•¼ í•©ë‹ˆë‹¤ (í˜„ì¬: {len(images[0][0])})")
    
    # external: (30, 3)
    if len(external) != 30:
        errors.append(f"external: ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 30ì´ì–´ì•¼ í•©ë‹ˆë‹¤ (í˜„ì¬: {len(external)})")
    elif len(external[0]) != 3:
        errors.append(f"external: ê° íƒ€ì„ìŠ¤í…ì€ 3ê°œ ê°’ì´ì–´ì•¼ í•©ë‹ˆë‹¤ (í˜„ì¬: {len(external[0])})")
    
    if errors:
        raise HTTPException(
            status_code=422,
            detail={
                "message": "ì…ë ¥ ë°ì´í„° shapeì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤",
                "errors": errors,
                "expected_shape": {
                    "sensors": [30, 8],
                    "images": [30, 60, 80],
                    "external": [30, 3]
                }
            }
        )


# ============== ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ ==============

@router.get("/health", response_model=HealthResponse)
async def health_check():
    """ì„œë²„ í—¬ìŠ¤ ì²´í¬"""
    return HealthResponse(
        status="healthy",
        model_loaded=model_loader.is_loaded(),
        device=str(model_loader.device) if model_loader.is_loaded() else "not loaded"
    )


@router.get("/model/info", response_model=ModelInfoResponse)
async def get_model_info():
    """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    if not model_loader.is_loaded():
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    info = model_loader.info
    return ModelInfoResponse(
        accuracy=info.get('best_accuracy', 0),
        parameters=info.get('parameters', 0),
        device=str(model_loader.device),
        input_shape={"sensors": [30, 8], "images": [30, 60, 80], "external": [30, 3]},
        output_classes=STATE_LABELS
    )


@router.post("/predict", response_model=PredictResponse)
async def predict(request: PredictRequest):
    """ë‹¨ì¼ ì˜ˆì¸¡"""
    if not model_loader.is_loaded():
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    # ì…ë ¥ ê²€ì¦ ì¶”ê°€
    validate_input_shape(request.sensors, request.images, request.external)
    
    try:
        processed = preprocess_input(
            request.sensors, request.images, request.external, request.normalize
        )
        
        device = model_loader.device
        sensors = torch.tensor(processed['sensors'], dtype=torch.float32).unsqueeze(0).to(device)
        images = torch.tensor(processed['images'], dtype=torch.float32).unsqueeze(0).to(device)
        external = torch.tensor(processed['external'], dtype=torch.float32).unsqueeze(0).to(device)
        
        result = model_loader.model.predict(sensors, images, external)
        
        pred = result['prediction'].item()
        conf = result['confidence'].item()
        probs = result['probabilities'].squeeze().tolist()
        
        return PredictResponse(
            prediction=pred,
            label=STATE_LABELS[pred],
            confidence=round(conf, 4),
            probabilities=[round(p, 4) for p in probs]
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/predict/batch", response_model=BatchPredictResponse)
async def predict_batch(request: BatchPredictRequest):
    """ë°°ì¹˜ ì˜ˆì¸¡"""
    if not model_loader.is_loaded():
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    # ê° ì•„ì´í…œ ì…ë ¥ ê²€ì¦
    for i, item in enumerate(request.items):
        try:
            validate_input_shape(item.sensors, item.images, item.external)
        except HTTPException as e:
            raise HTTPException(
                status_code=422,
                detail={"message": f"items[{i}] ê²€ì¦ ì‹¤íŒ¨", "errors": e.detail}
            )
    
    try:
        device = model_loader.device
        all_sensors, all_images, all_external = [], [], []
        
        for item in request.items:
            processed = preprocess_input(
                item.sensors, item.images, item.external, item.normalize
            )
            all_sensors.append(processed['sensors'])
            all_images.append(processed['images'])
            all_external.append(processed['external'])
        
        sensors = torch.tensor(np.stack(all_sensors), dtype=torch.float32).to(device)
        images = torch.tensor(np.stack(all_images), dtype=torch.float32).to(device)
        external = torch.tensor(np.stack(all_external), dtype=torch.float32).to(device)
        
        result = model_loader.model.predict(sensors, images, external)
        
        results = []
        for i in range(len(request.items)):
            pred = result['prediction'][i].item()
            results.append(PredictResponse(
                prediction=pred,
                label=STATE_LABELS[pred],
                confidence=round(result['confidence'][i].item(), 4),
                probabilities=[round(p, 4) for p in result['probabilities'][i].tolist()]
            ))
        
        return BatchPredictResponse(results=results, total=len(results))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Batch prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============== í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸ (NEW) ==============

@router.get("/predict/test", response_model=PredictResponse)
async def predict_test():
    """
    ğŸ§ª ë”ë¯¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    
    Swagger UIì—ì„œ ì‰½ê²Œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë„ë¡ ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - sensors: ì •ìƒ ë²”ìœ„ì˜ ì„¼ì„œê°’ (NTC=25â„ƒ, PM=10~20, CT=50A)
    - images: ê· ì¼í•œ ì—´í™”ìƒ (30â„ƒ)
    - external: ì •ìƒ í™˜ê²½ (25â„ƒ, 50%, 500lux)
    """
    if not model_loader.is_loaded():
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„± (ì •ìƒ ìƒíƒœ ì‹œë®¬ë ˆì´ì…˜)
    dummy_sensors = [[25, 10, 15, 20, 50, 50, 50, 50]] * 30  # (30, 8)
    dummy_images = [[[30.0] * 80 for _ in range(60)] for _ in range(30)]  # (30, 60, 80)
    dummy_external = [[25, 50, 500]] * 30  # (30, 3)
    
    try:
        processed = preprocess_input(dummy_sensors, dummy_images, dummy_external, normalize=True)
        
        device = model_loader.device
        sensors = torch.tensor(processed['sensors'], dtype=torch.float32).unsqueeze(0).to(device)
        images = torch.tensor(processed['images'], dtype=torch.float32).unsqueeze(0).to(device)
        external = torch.tensor(processed['external'], dtype=torch.float32).unsqueeze(0).to(device)
        
        result = model_loader.model.predict(sensors, images, external)
        
        pred = result['prediction'].item()
        conf = result['confidence'].item()
        probs = result['probabilities'].squeeze().tolist()
        
        return PredictResponse(
            prediction=pred,
            label=STATE_LABELS[pred],
            confidence=round(conf, 4),
            probabilities=[round(p, 4) for p in probs]
        )
    except Exception as e:
        logger.error(f"Test prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/predict/test/degraded", response_model=PredictResponse)
async def predict_test_degraded():
    """
    ğŸ§ª ì—´í™” ìƒíƒœ ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸
    
    ë¹„ì •ìƒì ì¸ ì„¼ì„œê°’ìœ¼ë¡œ ì—´í™” ìƒíƒœë¥¼ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.
    - sensors: ë†’ì€ ì˜¨ë„(80â„ƒ), ë†’ì€ ë¯¸ì„¸ë¨¼ì§€, ë†’ì€ ì „ë¥˜
    - images: ê³ ì˜¨ ì—´í™”ìƒ (80â„ƒ)
    - external: ê³ ì˜¨ í™˜ê²½ (40â„ƒ)
    """
    if not model_loader.is_loaded():
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    # ì—´í™” ìƒíƒœ ì‹œë®¬ë ˆì´ì…˜
    dummy_sensors = [[80, 200, 300, 400, 150, 150, 150, 150]] * 30  # ë†’ì€ ê°’ë“¤
    dummy_images = [[[80.0] * 80 for _ in range(60)] for _ in range(30)]  # ê³ ì˜¨
    dummy_external = [[40, 80, 200]] * 30  # ê³ ì˜¨/ê³ ìŠµë„
    
    try:
        processed = preprocess_input(dummy_sensors, dummy_images, dummy_external, normalize=True)
        
        device = model_loader.device
        sensors = torch.tensor(processed['sensors'], dtype=torch.float32).unsqueeze(0).to(device)
        images = torch.tensor(processed['images'], dtype=torch.float32).unsqueeze(0).to(device)
        external = torch.tensor(processed['external'], dtype=torch.float32).unsqueeze(0).to(device)
        
        result = model_loader.model.predict(sensors, images, external)
        
        pred = result['prediction'].item()
        conf = result['confidence'].item()
        probs = result['probabilities'].squeeze().tolist()
        
        return PredictResponse(
            prediction=pred,
            label=STATE_LABELS[pred],
            confidence=round(conf, 4),
            probabilities=[round(p, 4) for p in probs]
        )
    except Exception as e:
        logger.error(f"Degraded test prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))