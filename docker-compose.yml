version: '3.8'

services:
  # ============================================
  # ML API 서비스
  # ============================================
  ml-api:
    build:
      context: ./conveyorguard-api
      dockerfile: Dockerfile
    container_name: conveyorguard-ml-api
    ports:
      - "8000:8000"
    volumes:
      - ./data/models:/app/models:ro
    env_file:
      - ./conveyorguard-api/.env
    environment:
      - MODEL_PATH=/app/models/baseline_cnn_model.pt
      - LOG_LEVEL=INFO
      - LLM_API_URL=http://llm-service:8001
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      llm-service:
        condition: service_healthy
    restart: unless-stopped

  # ============================================
  # LLM 진단 서비스
  # ============================================
  llm-service:
    build:
      context: ./llm-service
      dockerfile: Dockerfile
    container_name: conveyorguard-llm
    ports:
      - "8001:8001"
    env_file:
      - ./llm-service/.env
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ============================================
  # MLFlow (optional — 필요 시 활성화)
  # ============================================
  # mlflow:
  #   image: ghcr.io/mlflow/mlflow:v2.9.2
  #   container_name: conveyorguard-mlflow
  #   ports:
  #     - "5000:5000"
  #   volumes:
  #     - mlruns:/mlruns
  #     - ./data/models:/models
  #   environment:
  #     - MLFLOW_TRACKING_URI=http://0.0.0.0:5000
  #   command: >
  #     mlflow server
  #     --host 0.0.0.0
  #     --port 5000
  #     --backend-store-uri sqlite:///mlruns/mlflow.db
  #     --default-artifact-root /mlruns
  #   restart: unless-stopped

# volumes:
#   mlruns:
#     driver: local
