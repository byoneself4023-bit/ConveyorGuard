{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14685395,"sourceType":"datasetVersion","datasetId":9381506}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ConveyorGuard - 05. LLM ì§„ë‹¨ ë¹„êµ + LangGraph ë©€í‹° ì—ì´ì „íŠ¸\n\n**3ì¢… LLM ë¹„êµ + LangGraph ì›Œí¬í”Œë¡œìš°**\n\n- **Accelerator: GPU T4 x2**\n- ì…ë ¥: `test_data.joblib`\n- ì¶œë ¥: `llm_comparison_results.csv`, `langgraph_report.md`, `llm_comparison.html`\n- **Secrets í•„ìš”**: `GEMINI_API_KEY`, `HF_TOKEN`\n\n---\n\n### Pipeline\n> `00_eda` â†’ `01_preprocess` â†’ `02/03`(ë³‘ë ¬) â†’ `04_dl_tuning` â†’ **`05_llm_comparison`**\n\n| ì…ë ¥ ë°ì´í„°ì…‹ | ì¶œë ¥ | Accelerator |\n|--------------|------|-------------|\n| `conveyorguard-preprocess` (01 ì¶œë ¥) | `llm_comparison_results.csv` | GPU T4 x2 |","metadata":{}},{"cell_type":"markdown","source":"## 1. í™˜ê²½ ì„¤ì •","metadata":{}},{"cell_type":"code","source":"# íŒ¨í‚¤ì§€ ì„¤ì¹˜\n!pip install google-generativeai langgraph langchain-google-genai transformers accelerate plotly -q\n\nimport os, sys, time, json, base64, gc\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport torch\nfrom pathlib import Path\nfrom PIL import Image\nfrom io import BytesIO\nfrom typing import TypedDict, List\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Plotly ì„¤ì •\nimport plotly.io as pio\npio.renderers.default = 'iframe'\n\n# í™˜ê²½ ì •ë³´\nprint(f\"Python: {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA: {torch.version.cuda}\")\n\n# GPU í™•ì¸ (T4 x2)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"\\nğŸ–¥ï¸ Device: {device}\")\nif torch.cuda.is_available():\n    for i in range(torch.cuda.device_count()):\n        props = torch.cuda.get_device_properties(i)\n        print(f\"   GPU {i}: {props.name} ({getattr(props, 'total_memory', None) or getattr(props, 'total_mem', 0) / 1024**3:.1f} GB)\")\n    print(f\"   Total GPUs: {torch.cuda.device_count()}\")\n\n# ê²½ë¡œ ì„¤ì • (fallback ì§€ì›)\nDATA_DIR = Path('/kaggle/input/conveyorguard-preprocess')\nif not DATA_DIR.exists():\n    DATA_DIR = Path('/kaggle/input/preprocess')\nOUTPUT_DIR = Path('/kaggle/working')\n\nprint(f\"\\nğŸ“‚ ë°ì´í„° í´ë”: {DATA_DIR}\")\nprint(f\"ğŸ“ ì¶œë ¥ í´ë”: {OUTPUT_DIR}\")\n\nprint(f\"\\nâœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")\n\n# ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì‹œì‘\n_notebook_start = time.time()","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:23:14.620206Z","iopub.execute_input":"2026-02-01T16:23:14.620508Z","iopub.status.idle":"2026-02-01T16:23:30.188652Z","shell.execute_reply.started":"2026-02-01T16:23:14.620482Z","shell.execute_reply":"2026-02-01T16:23:30.187872Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m157.4/157.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m490.2/490.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.6/212.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m342.0/342.0 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\nlangchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.2.7 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mPython: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\nPyTorch: 2.8.0+cu126\nCUDA: 12.6\n\nğŸ–¥ï¸ Device: cuda\n   GPU 0: Tesla T4 (15637086208.0 GB)\n   GPU 1: Tesla T4 (15637086208.0 GB)\n   Total GPUs: 2\n\nğŸ“‚ ë°ì´í„° í´ë”: /kaggle/input/conveyorguard-preprocess\nğŸ“ ì¶œë ¥ í´ë”: /kaggle/working\n\nâœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 2. Gemini API ì„¤ì •","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    GEMINI_KEY = UserSecretsClient().get_secret('GEMINI_API_KEY')\n    genai.configure(api_key=GEMINI_KEY)\n    os.environ['GOOGLE_API_KEY'] = GEMINI_KEY\n    gemini_model = genai.GenerativeModel('gemini-2.5-flash')\n    print('âœ… Gemini API ì„¤ì • ì™„ë£Œ')\nexcept Exception as e:\n    GEMINI_KEY = None\n    gemini_model = None\n    print(f'âš ï¸ Gemini API í‚¤ ì—†ìŒ: {e}')","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:23:30.190121Z","iopub.execute_input":"2026-02-01T16:23:30.190481Z","iopub.status.idle":"2026-02-01T16:23:39.266895Z","shell.execute_reply.started":"2026-02-01T16:23:30.190443Z","shell.execute_reply":"2026-02-01T16:23:39.266026Z"},"trusted":true},"outputs":[{"name":"stdout","text":"âœ… Gemini API ì„¤ì • ì™„ë£Œ\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 3. HuggingFace ëª¨ë¸ ë¡œë“œ","metadata":{}},{"cell_type":"code","source":"# HuggingFace ë¡œê·¸ì¸ (ë§¨ ì•ì— ì¶”ê°€!)\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nHF_TOKEN = UserSecretsClient().get_secret('HF_TOKEN')\nlogin(token=HF_TOKEN)\nprint('âœ… HuggingFace ë¡œê·¸ì¸ ì™„ë£Œ')\n\n# ê¸°ì¡´ ì½”ë“œ\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(\"ğŸ”„ gemma-3-4b-it ë¡œë”© (2-3ë¶„)...\")\ngemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\ngemma_model = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-3-4b-it\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\nprint(\"âœ… gemma-3-4b-it ë¡œë“œ ì™„ë£Œ!\")","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:23:39.268021Z","iopub.execute_input":"2026-02-01T16:23:39.268527Z","iopub.status.idle":"2026-02-01T16:24:41.590853Z","shell.execute_reply.started":"2026-02-01T16:23:39.268501Z","shell.execute_reply":"2026-02-01T16:24:41.590111Z"},"trusted":true},"outputs":[{"name":"stdout","text":"âœ… HuggingFace ë¡œê·¸ì¸ ì™„ë£Œ\nğŸ”„ gemma-3-4b-it ë¡œë”© (2-3ë¶„)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87cb9130fc4a46cab9fe92da011da017"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e8f3a5bda8846b683cbbc163567ad6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ed54dce394747c5bb53b8076fccebc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2912cb75ec54f1f9762238d4e7d21be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db4b65e9466d41cb996a6dfab97a3350"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"310559ff8eca474eb5a966360535b131"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n2026-02-01 16:23:52.727359: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769963032.966559      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769963033.040977      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769963033.614838      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769963033.614873      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769963033.614876      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769963033.614879      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"476647eafe0848a0a3f1ca10f5afa5cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75e644d529694063b791def6546988e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5025e7f99bef49eda3203c351d1e5ce9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94a37965e5924320801c27d2598bb7c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b44f3e45aacb413a942acf91704490e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51f60750dd0547f4a237d0251710bc30"}},"metadata":{}},{"name":"stdout","text":"âœ… gemma-3-4b-it ë¡œë“œ ì™„ë£Œ!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 2. Qwen2.5-3B (GPU 1ì— ë°°ì¹˜)\nprint('ğŸ”„ Qwen2.5-3B-Instruct ë¡œë”©...')\nqwen_tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-3B-Instruct')\nqwen_model = AutoModelForCausalLM.from_pretrained(\n    'Qwen/Qwen2.5-3B-Instruct',\n    torch_dtype=torch.float16,\n    device_map={'': 1} if torch.cuda.device_count() > 1 else 'auto'  # GPU 1ì— ë°°ì¹˜\n)\nprint(f'âœ… Qwen2.5-3B-Instruct ë¡œë“œ ì™„ë£Œ! (GPU {1 if torch.cuda.device_count() > 1 else 0})')\n\n# âš¡ 2GPU ìµœì í™”: Gemma â†’ GPU0, Qwen â†’ GPU1 (ë³‘ë ¬ ì¶”ë¡  ê°€ëŠ¥)\nprint(f'\\nğŸ–¥ï¸ ëª¨ë¸ ë°°ì¹˜:')\nprint(f'   Gemma-3-4B: GPU 0')\nprint(f'   Qwen2.5-3B: GPU {1 if torch.cuda.device_count() > 1 else 0}')\nprint(f'   Gemini: API (ë„¤íŠ¸ì›Œí¬)')","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:24:41.592661Z","iopub.execute_input":"2026-02-01T16:24:41.593333Z","iopub.status.idle":"2026-02-01T16:25:06.919427Z","shell.execute_reply.started":"2026-02-01T16:24:41.593305Z","shell.execute_reply":"2026-02-01T16:25:06.918765Z"},"trusted":true},"outputs":[{"name":"stdout","text":"ğŸ”„ Qwen2.5-3B-Instruct ë¡œë”©...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acde48b2d46641c7a49a79efa91f4322"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"949c3653afc44ffda048cf550bf65af5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"008a6f95574a4c82837feea9dc488257"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbb32632c7c24884a6061321836aee15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"253988c6d91149d98a625aa1a18329b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c08a2b94da1472abd9929d6e46e74a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdbc0b5e7c4b44f0a4f1441ab4f0d300"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4231a28eb62451fb4ed7d64e9fd9a58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5aae7621f034d7a82cd95f62b79d936"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f94d26cc069046748a3b3a3c8a7720c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d074725dddee4c2dbf019b36c9a15dab"}},"metadata":{}},{"name":"stdout","text":"âœ… Qwen2.5-3B-Instruct ë¡œë“œ ì™„ë£Œ! (GPU 1)\n\nğŸ–¥ï¸ ëª¨ë¸ ë°°ì¹˜:\n   Gemma-3-4B: GPU 0\n   Qwen2.5-3B: GPU 1\n   Gemini: API (ë„¤íŠ¸ì›Œí¬)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 4. LLM í•¨ìˆ˜ ì •ì˜","metadata":{}},{"cell_type":"code","source":"def call_gemini(prompt):\n    if not gemini_model:\n        return None, 0\n    start = time.time()\n    try:\n        resp = gemini_model.generate_content(prompt)\n        return resp.text, time.time() - start\n    except Exception as e:\n        print(f'Gemini ì˜¤ë¥˜: {e}')\n        return None, 0\n\ndef call_gemma(prompt, max_new_tokens=150):\n    \"\"\"ì´ë¯¸ ë¡œë“œëœ gemma_model ì‚¬ìš©\"\"\"\n    start = time.time()\n    try:\n        inputs = gemma_tokenizer(prompt, return_tensors=\"pt\", padding=True).to(gemma_model.device)\n        if gemma_tokenizer.pad_token is None:\n            gemma_tokenizer.pad_token = gemma_tokenizer.eos_token\n        with torch.no_grad():\n            outputs = gemma_model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                do_sample=False,\n                num_beams=1,\n                pad_token_id=gemma_tokenizer.pad_token_id or gemma_tokenizer.eos_token_id\n            )\n        response = gemma_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n        return response.strip(), time.time() - start\n    except Exception as e:\n        print(f'Gemma ì˜¤ë¥˜: {e}')\n        return None, 0\n\ndef call_qwen(prompt, max_new_tokens=150):\n    \"\"\"ì´ë¯¸ ë¡œë“œëœ qwen_model ì‚¬ìš©\"\"\"\n    start = time.time()\n    try:\n        inputs = qwen_tokenizer(prompt, return_tensors=\"pt\", padding=True).to(qwen_model.device)\n        if qwen_tokenizer.pad_token is None:\n            qwen_tokenizer.pad_token = qwen_tokenizer.eos_token\n        with torch.no_grad():\n            outputs = qwen_model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                do_sample=False,\n                num_beams=1,\n                pad_token_id=qwen_tokenizer.pad_token_id or qwen_tokenizer.eos_token_id\n            )\n        response = qwen_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n        return response.strip(), time.time() - start\n    except Exception as e:\n        print(f'Qwen ì˜¤ë¥˜: {e}')\n        return None, 0\n\nprint('âœ… LLM í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ (ë¡œë“œëœ ëª¨ë¸ ì§ì ‘ ì‚¬ìš©)')","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:25:06.920366Z","iopub.execute_input":"2026-02-01T16:25:06.920932Z","iopub.status.idle":"2026-02-01T16:25:06.932033Z","shell.execute_reply.started":"2026-02-01T16:25:06.920904Z","shell.execute_reply":"2026-02-01T16:25:06.931269Z"},"trusted":true},"outputs":[{"name":"stdout","text":"âœ… LLM í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ (ë¡œë“œëœ ëª¨ë¸ ì§ì ‘ ì‚¬ìš©)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 5. LangGraph ë©€í‹° ì—ì´ì „íŠ¸ ì •ì˜","metadata":{}},{"cell_type":"code","source":"from langgraph.graph import StateGraph, END\n\n# LangGraphìš© LLM (Gemini)\nllm = ChatGoogleGenerativeAI(model='gemini-2.5-flash', temperature=0.7)\n\n# State ì •ì˜\nclass DiagnosisState(TypedDict):\n    sensor_data: dict\n    thermal_data: dict\n    analysis: str\n    diagnosis: str\n    advice: str\n    review: str\n    review_count: int\n    final_report: str\n\nprint('âœ… State ì •ì˜ ì™„ë£Œ')","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:25:06.932919Z","iopub.execute_input":"2026-02-01T16:25:06.933159Z","iopub.status.idle":"2026-02-01T16:25:15.699005Z","shell.execute_reply.started":"2026-02-01T16:25:06.933135Z","shell.execute_reply":"2026-02-01T16:25:15.698271Z"},"trusted":true},"outputs":[{"name":"stdout","text":"âœ… State ì •ì˜ ì™„ë£Œ\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ì—ì´ì „íŠ¸ ë…¸ë“œ ì •ì˜\n\ndef analyzer_node(state: DiagnosisState) -> DiagnosisState:\n    print('--- [1/4] Analyzer ---')\n    s, t = state['sensor_data'], state['thermal_data']\n    prompt = f'''ì œì¡° ì„¤ë¹„ ë°ì´í„° ë¶„ì„:\nì„¼ì„œ: NTC {s[\"ntc\"]:.1f}Â°C (ì •ìƒ:40-60), CT1 {s[\"ct1\"]:.1f}A (ì •ìƒ:20-40), PM2.5 {s[\"pm25\"]:.1f} (ì •ìƒ:0-35)\nì—´í™”ìƒ: ìµœê³  {t[\"max\"]:.1f}Â°C, í‰ê·  {t[\"mean\"]:.1f}Â°C, í¸ì°¨ {t[\"std\"]:.2f}\nê° ì„¼ì„œê°’ì˜ ì •ìƒ/ì´ìƒ ì—¬ë¶€ë¥¼ ë¶„ì„í•˜ì„¸ìš”.'''\n    resp = llm.invoke(prompt)\n    return {'analysis': resp.content}\n\ndef diagnoser_node(state: DiagnosisState) -> DiagnosisState:\n    print('--- [2/4] Diagnoser ---')\n    prompt = f'''ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ìƒ ì›ì¸ì„ 3ê°€ì§€ ì´ë‚´ë¡œ ì¶”ì •í•˜ì„¸ìš”.\në¶„ì„ ê²°ê³¼: {state[\"analysis\"]}'''\n    resp = llm.invoke(prompt)\n    return {'diagnosis': resp.content}\n\ndef advisor_node(state: DiagnosisState) -> DiagnosisState:\n    print('--- [3/4] Advisor ---')\n    prompt = f'''ë‹¤ìŒ ì§„ë‹¨ì— ëŒ€í•œ ì¡°ì¹˜ ì‚¬í•­ì„ ì œì•ˆí•˜ì„¸ìš”.\nì§„ë‹¨: {state[\"diagnosis\"]}\ní˜•ì‹: 1. ì¦‰ì‹œ ì¡°ì¹˜ 2. ì˜ˆë°© ì¡°ì¹˜ 3. ì ê²€ ì£¼ê¸°'''\n    resp = llm.invoke(prompt)\n    return {'advice': resp.content}\n\ndef reviewer_node(state: DiagnosisState) -> DiagnosisState:\n    print('--- [4/4] Reviewer ---')\n    prompt = f'''ì§„ë‹¨ ë¦¬í¬íŠ¸ ê²€í† :\n[ë¶„ì„] {state[\"analysis\"][:300]}\n[ì›ì¸] {state[\"diagnosis\"][:300]}\n[ì¡°ì¹˜] {state[\"advice\"][:300]}\nì¶©ë¶„í•˜ë©´ APPROVE, ìˆ˜ì • í•„ìš”í•˜ë©´ REVISEë¥¼ ë§ˆì§€ë§‰ ì¤„ì— í¬í•¨.'''\n    resp = llm.invoke(prompt)\n    return {'review': resp.content, 'review_count': state.get('review_count', 0) + 1}\n\nprint('âœ… ì—ì´ì „íŠ¸ ë…¸ë“œ ì •ì˜ ì™„ë£Œ')","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:25:15.699990Z","iopub.execute_input":"2026-02-01T16:25:15.700267Z","iopub.status.idle":"2026-02-01T16:25:15.708616Z","shell.execute_reply.started":"2026-02-01T16:25:15.700242Z","shell.execute_reply":"2026-02-01T16:25:15.707827Z"},"trusted":true},"outputs":[{"name":"stdout","text":"âœ… ì—ì´ì „íŠ¸ ë…¸ë“œ ì •ì˜ ì™„ë£Œ\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ì¡°ê±´ë¶€ ì—£ì§€ ë° ì›Œí¬í”Œë¡œìš° êµ¬ì¶•\n\ndef should_continue(state: DiagnosisState) -> str:\n    review = state.get('review', '')  # ë²„ê·¸ ìˆ˜ì •: ë¯¸ì •ì˜ ë³€ìˆ˜ â†’ stateì—ì„œ ì¡°íšŒ\n    \n    if state['review_count'] >= 2:\n        print('ìµœëŒ€ ìˆ˜ì • íšŸìˆ˜ â†’ ì¢…ë£Œ')\n        return 'end'\n    \n    if 'APPROVE' in str(review).upper():\n        print('ìŠ¹ì¸(APPROVE) â†’ ì¢…ë£Œ')\n        return 'end'\n    \n    print('ìˆ˜ì •(REVISE) â†’ Diagnoser')\n    return 'revise'\n\ndef finalize_node(state: DiagnosisState) -> DiagnosisState:\n    report = f'''# ConveyorGuard ì§„ë‹¨ ë¦¬í¬íŠ¸\n\n## 1. ë°ì´í„° ë¶„ì„\n\n{state[\"analysis\"]}\n\n## 2. ì›ì¸ ì¶”ì •\n\n{state[\"diagnosis\"]}\n\n## 3. ì¡°ì¹˜ ì‚¬í•­\n\n{state[\"advice\"]}\n\n## 4. ê²€í†  ì˜ê²¬\n\n{state[\"review\"]}\n\n---\n\nìˆ˜ì • íšŸìˆ˜: {state[\"review_count\"]}íšŒ\n'''\n    \n    return {'final_report': report}\n\n# ì›Œí¬í”Œë¡œìš° êµ¬ì¶•\n\nworkflow = StateGraph(DiagnosisState)\n\nworkflow.add_node('analyzer', analyzer_node)\nworkflow.add_node('diagnoser', diagnoser_node)\nworkflow.add_node('advisor', advisor_node)\nworkflow.add_node('reviewer', reviewer_node)\nworkflow.add_node('finalize', finalize_node)\n\nworkflow.set_entry_point('analyzer')\nworkflow.add_edge('analyzer', 'diagnoser')\nworkflow.add_edge('diagnoser', 'advisor')\nworkflow.add_edge('advisor', 'reviewer')\nworkflow.add_conditional_edges('reviewer', should_continue, {'revise': 'diagnoser', 'end': 'finalize'})\nworkflow.add_edge('finalize', END)\n\nlanggraph_app = workflow.compile()\n\nprint('\\nâœ… LangGraph ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ì™„ë£Œ!')","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:25:15.709667Z","iopub.execute_input":"2026-02-01T16:25:15.710063Z","iopub.status.idle":"2026-02-01T16:25:15.737400Z","shell.execute_reply.started":"2026-02-01T16:25:15.710028Z","shell.execute_reply":"2026-02-01T16:25:15.736457Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nâœ… LangGraph ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ì™„ë£Œ!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## 6. ë°ì´í„° ë¡œë“œ","metadata":{}},{"cell_type":"code","source":"print('ë°ì´í„° ë¡œë“œ...')\ntest_data = joblib.load(DATA_DIR / 'test_data.joblib')\nprint(f'Test: {test_data[\"sensors\"].shape[0]:,}ê°œ')\n\n# ì„¼ì„œ ìˆœì„œ: ['NTC', 'PM1.0', 'PM2.5', 'PM10', 'CT1', 'CT2', 'CT3', 'CT4']\nlabels = ['ì •ìƒ', 'ê²½ë¯¸', 'ì¤‘ê°„', 'ì‹¬ê°']\nsamples = []\nfor i in range(4):\n    idx = np.where(test_data['labels'] == i)[0][0]\n    samples.append({\n        'idx': idx, 'label': labels[i],\n        'sensors': test_data['sensors'][idx],\n        'image': test_data['images'][idx][-1]\n    })\nprint(f'ìƒ˜í”Œ: {len(samples)}ê°œ (í´ë˜ìŠ¤ë³„ 1ê°œ)')","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:25:15.738489Z","iopub.execute_input":"2026-02-01T16:25:15.738877Z","iopub.status.idle":"2026-02-01T16:25:24.530131Z","shell.execute_reply.started":"2026-02-01T16:25:15.738851Z","shell.execute_reply":"2026-02-01T16:25:24.529207Z"},"trusted":true},"outputs":[{"name":"stdout","text":"ë°ì´í„° ë¡œë“œ...\nTest: 1,608ê°œ\nìƒ˜í”Œ: 4ê°œ (í´ë˜ìŠ¤ë³„ 1ê°œ)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 7. Part A: LLM 3ì¢… ë¹„êµ","metadata":{}},{"cell_type":"code","source":"print('='*60)\nprint('Part A: LLM 3ì¢… ë¹„êµ')\nprint('='*60)\n\nresults = []\n\nfor i, s in enumerate(samples):\n    print(f'\\n[{i+1}/4] {s[\"label\"]} ìƒ˜í”Œ')\n    \n    # ì„¼ì„œê°’ ì¶”ì¶œ\n    ntc = float(s['sensors'][-1, 0])\n    ct1 = float(s['sensors'][-1, 4])\n    pm25 = float(s['sensors'][-1, 2])\n    t_max = float(s['image'].max())\n    t_mean = float(s['image'].mean())\n    \n    prompt = f'''ì œì¡° ì„¤ë¹„ OHT ì§„ë‹¨:\n- NTC ì˜¨ë„: {ntc:.1f}Â°C\n- CT1 ì „ë¥˜: {ct1:.1f}A  \n- PM2.5: {pm25:.1f}\n- ì—´í™”ìƒ ìµœê³ : {t_max:.1f}Â°C, í‰ê· : {t_mean:.1f}Â°C\nì¥ë¹„ ìƒíƒœë¥¼ ì§„ë‹¨í•˜ê³  ì´ìƒ ì‹œ ì¡°ì¹˜ì‚¬í•­ì„ 100ì ì´ë‚´ë¡œ ë‹µí•˜ì„¸ìš”.'''\n    \n    # 1. Gemini\n    print('  Gemini...', end=' ', flush=True)\n    g_resp, g_time = call_gemini(prompt)\n    print(f'{g_time:.1f}s')\n    \n    # 2. Gemma (ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©)\n    print('  Gemma-3...', end=' ', flush=True)\n    m_resp, m_time = call_gemma(prompt, 150)\n    print(f'{m_time:.1f}s')\n    \n    # 3. Qwen (ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©)\n    print('  Qwen2.5...', end=' ', flush=True)\n    q_resp, q_time = call_qwen(prompt, 150)\n    print(f'{q_time:.1f}s')\n    \n    results.append({\n        'label': s['label'], 'ntc': ntc, 't_max': t_max,\n        'gemini_resp': g_resp, 'gemini_time': g_time,\n        'gemma_resp': m_resp, 'gemma_time': m_time,\n        'qwen_resp': q_resp, 'qwen_time': q_time\n    })\n\nprint('\\nâœ… Part A ì™„ë£Œ!')","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:25:24.532645Z","iopub.execute_input":"2026-02-01T16:25:24.532974Z","iopub.status.idle":"2026-02-01T16:27:27.589424Z","shell.execute_reply.started":"2026-02-01T16:25:24.532943Z","shell.execute_reply":"2026-02-01T16:27:27.588443Z"},"trusted":true},"outputs":[{"name":"stdout","text":"============================================================\nPart A: LLM 3ì¢… ë¹„êµ\n============================================================\n\n[1/4] ì •ìƒ ìƒ˜í”Œ\n  Gemini... 8.3s\n  Gemma-3... ","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"15.4s\n  Qwen2.5... ","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"8.2s\n\n[2/4] ê²½ë¯¸ ìƒ˜í”Œ\n  Gemini... 9.4s\n  Gemma-3... 14.4s\n  Qwen2.5... 8.1s\n\n[3/4] ì¤‘ê°„ ìƒ˜í”Œ\n  Gemini... 7.5s\n  Gemma-3... 14.4s\n  Qwen2.5... 8.0s\n\n[4/4] ì‹¬ê° ìƒ˜í”Œ\n  Gemini... 6.7s\n  Gemma-3... 14.4s\n  Qwen2.5... 8.2s\n\nâœ… Part A ì™„ë£Œ!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## 8. Part B: LangGraph ë©€í‹° ì—ì´ì „íŠ¸ ì§„ë‹¨","metadata":{}},{"cell_type":"code","source":"print('='*60)\nprint('Part B: LangGraph ë©€í‹° ì—ì´ì „íŠ¸')\nprint('='*60)\n\n# ì‹¬ê° ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸\ns = samples[3]\nprint(f'\\ní…ŒìŠ¤íŠ¸: {s[\"label\"]} ìƒ˜í”Œ')\n\nntc = float(s['sensors'][-1, 0])\nct1 = float(s['sensors'][-1, 4])\npm25 = float(s['sensors'][-1, 2])\nt_max = float(s['image'].max())\nt_mean = float(s['image'].mean())\nt_std = float(s['image'].std())\n\ninputs = {\n    'sensor_data': {'ntc': ntc, 'ct1': ct1, 'pm25': pm25},\n    'thermal_data': {'max': t_max, 'mean': t_mean, 'std': t_std},\n    'review_count': 0\n}\n\nprint('\\nLangGraph ì‹¤í–‰...')\nstart = time.time()\nfinal_report = \"\"\nfor output in langgraph_app.stream(inputs, {'recursion_limit': 10}):\n    for node, state in output.items():\n        if node == 'finalize':\n            final_report = state.get('final_report', '')\nelapsed = time.time() - start\n\nprint(f'\\nâ±ï¸ ì´ ì†Œìš”: {elapsed:.1f}ì´ˆ')\nprint('\\nâœ… Part B ì™„ë£Œ!')","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:27:27.590728Z","iopub.execute_input":"2026-02-01T16:27:27.590993Z","iopub.status.idle":"2026-02-01T16:28:59.072956Z","shell.execute_reply.started":"2026-02-01T16:27:27.590968Z","shell.execute_reply":"2026-02-01T16:28:59.072114Z"},"trusted":true},"outputs":[{"name":"stdout","text":"============================================================\nPart B: LangGraph ë©€í‹° ì—ì´ì „íŠ¸\n============================================================\n\ní…ŒìŠ¤íŠ¸: ì‹¬ê° ìƒ˜í”Œ\n\nLangGraph ì‹¤í–‰...\n--- [1/4] Analyzer ---\n--- [2/4] Diagnoser ---\n--- [3/4] Advisor ---\n--- [4/4] Reviewer ---\nìˆ˜ì •(REVISE) â†’ Diagnoser\n--- [2/4] Diagnoser ---\n--- [3/4] Advisor ---\n--- [4/4] Reviewer ---\nìµœëŒ€ ìˆ˜ì • íšŸìˆ˜ â†’ ì¢…ë£Œ\n\nâ±ï¸ ì´ ì†Œìš”: 91.5ì´ˆ\n\nâœ… Part B ì™„ë£Œ!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## 9. ê²°ê³¼ ë¶„ì„","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(results)\n\nprint('\\nğŸ“Š Part A: LLM ì‘ë‹µ ì‹œê°„ ë¹„êµ')\nprint('='*50)\nprint(f'Gemini 2.5 Flash : {df[\"gemini_time\"].mean():.2f}ì´ˆ (API)')\nprint(f'Gemma-3-4B       : {df[\"gemma_time\"].mean():.2f}ì´ˆ (ë¡œì»¬)')\nprint(f'Qwen2.5-3B       : {df[\"qwen_time\"].mean():.2f}ì´ˆ (ë¡œì»¬)')\n\nprint(f'\\nğŸ“Š Part B: LangGraph')\nprint('='*50)\nprint(f'ë©€í‹° ì—ì´ì „íŠ¸ ì´ ì†Œìš”: {elapsed:.1f}ì´ˆ')\nprint('ì›Œí¬í”Œë¡œìš°: Analyzer â†’ Diagnoser â†’ Advisor â†’ Reviewer')","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:28:59.073949Z","iopub.execute_input":"2026-02-01T16:28:59.074161Z","iopub.status.idle":"2026-02-01T16:28:59.097909Z","shell.execute_reply.started":"2026-02-01T16:28:59.074139Z","shell.execute_reply":"2026-02-01T16:28:59.097238Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nğŸ“Š Part A: LLM ì‘ë‹µ ì‹œê°„ ë¹„êµ\n==================================================\nGemini 2.5 Flash : 7.99ì´ˆ (API)\nGemma-3-4B       : 14.65ì´ˆ (ë¡œì»¬)\nQwen2.5-3B       : 8.12ì´ˆ (ë¡œì»¬)\n\nğŸ“Š Part B: LangGraph\n==================================================\në©€í‹° ì—ì´ì „íŠ¸ ì´ ì†Œìš”: 91.5ì´ˆ\nì›Œí¬í”Œë¡œìš°: Analyzer â†’ Diagnoser â†’ Advisor â†’ Reviewer\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ì‘ë‹µ ìƒ˜í”Œ ì¶œë ¥\nprint('\\nğŸ“ ìƒ˜í”Œë³„ ì‘ë‹µ (ì‹¬ê° ì¼€ì´ìŠ¤)')\nprint('='*60)\n\nr = results[3]  # ì‹¬ê°\nprint(f'[{r[\"label\"]}] NTC: {r[\"ntc\"]:.1f}Â°C, ì—´í™”ìƒMax: {r[\"t_max\"]:.1f}Â°C')\nprint(f'\\nâ–¶ Gemini 2.5 Flash ({r[\"gemini_time\"]:.1f}s):')\nprint((r[\"gemini_resp\"] or \"N/A\")[:300])\nprint(f'\\nâ–¶ Gemma-3-4B ({r[\"gemma_time\"]:.1f}s):')\nprint((r[\"gemma_resp\"] or \"N/A\")[:300])\nprint(f'\\nâ–¶ Qwen2.5-3B ({r[\"qwen_time\"]:.1f}s):')\nprint((r[\"qwen_resp\"] or \"N/A\")[:300])","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:28:59.098965Z","iopub.execute_input":"2026-02-01T16:28:59.099247Z","iopub.status.idle":"2026-02-01T16:28:59.105629Z","shell.execute_reply.started":"2026-02-01T16:28:59.099221Z","shell.execute_reply":"2026-02-01T16:28:59.104759Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nğŸ“ ìƒ˜í”Œë³„ ì‘ë‹µ (ì‹¬ê° ì¼€ì´ìŠ¤)\n============================================================\n[ì‹¬ê°] NTC: 0.3Â°C, ì—´í™”ìƒMax: 1.0Â°C\n\nâ–¶ Gemini 2.5 Flash (6.7s):\nì¥ë¹„ ë¯¸ê°€ë™ ë˜ëŠ” ì „ì› ì´ìƒ ì¶”ì •. CT ì „ë¥˜ 0.0A ë° ë‚®ì€ ì˜¨ë„ê°€ ì§€í‘œ. ì „ì› ë° ì¥ë¹„ ì‘ë™ ìƒíƒœë¥¼ ì ê²€í•˜ê³  í•„ìš” ì‹œ ì „ì› ê³µê¸‰ ì¡°ì¹˜.\n\nâ–¶ Gemma-3-4B (14.4s):\nN/A\n\nâ–¶ Qwen2.5-3B (8.2s):\nì¥ë¹„ ì˜¨ë„ ì œì–´ ë¶ˆëŸ‰, CT1 ê³¼ë¶€í•˜. PM2.5 ì €ì¡°. ì—´í™”ìƒ ì„¼ì„œ ì˜¤ì‘ë™. ì •ê¸°ì ê²€ ë° ë³´ìˆ˜ í•„ìš”. \n\n(ì´í•´í•œ ê²ƒ ê°™ì•„ìš”. ì¥ë¹„ì˜ ì˜¨ë„ ì œì–´ ë¶ˆëŸ‰ê³¼ CT1 ê³¼ë¶€í•˜, PM2.5 ì„¼ì„œ ì €ì¡°, ê·¸ë¦¬ê³  ì—´í™”ìƒ ì„¼ì„œ ì˜¤ì‘ë™ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì •ê¸°ì ì¸ ì ê²€ê³¼ ë³´ìˆ˜ë¥¼ í†µí•´ ë¬¸ì œ í•´ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.) \n\nìœ„ì˜ ë‹µë³€ì´ ì›í•˜ì‹œëŠ” ê²°ê³¼ë¥¼ ì œê³µí•˜ì˜€ëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”. ì•„ë‹ˆë©´ ë” êµ¬ì²´ì ìœ¼ë¡œ ì›í•˜ì‹œëŠ” ë‹µë³€ì„ ì•Œë ¤ì£¼ì„¸ìš”. \n\nìœ„ì˜ ë‹µë³€ì´ ì›\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# LangGraph ë¦¬í¬íŠ¸\nprint('\\nğŸ“‹ LangGraph ìµœì¢… ë¦¬í¬íŠ¸')\nprint('='*60)\nprint(final_report[:2000] if final_report else \"ë¦¬í¬íŠ¸ ì—†ìŒ\")","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:28:59.106941Z","iopub.execute_input":"2026-02-01T16:28:59.107509Z","iopub.status.idle":"2026-02-01T16:28:59.116345Z","shell.execute_reply.started":"2026-02-01T16:28:59.107474Z","shell.execute_reply":"2026-02-01T16:28:59.115492Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nğŸ“‹ LangGraph ìµœì¢… ë¦¬í¬íŠ¸\n============================================================\n# ConveyorGuard ì§„ë‹¨ ë¦¬í¬íŠ¸\n\n## 1. ë°ì´í„° ë¶„ì„\n\nì œê³µí•´ì£¼ì‹  ì œì¡° ì„¤ë¹„ ì„¼ì„œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ì„¼ì„œê°’ì˜ ì •ìƒ/ì´ìƒ ì—¬ë¶€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.\n\n---\n\n### **ì„¼ì„œ ë°ì´í„° ë¶„ì„ ê²°ê³¼**\n\n1.  **NTC (ì˜¨ë„ ì„¼ì„œ)**\n    *   **í˜„ì¬ ê°’:** 0.3Â°C\n    *   **ì •ìƒ ë²”ìœ„:** 40-60Â°C\n    *   **ë¶„ì„:** í˜„ì¬ ê°’ 0.3Â°CëŠ” ì •ìƒ ë²”ìœ„(40-60Â°C)ì— ë¹„í•´ **ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤.**\n    *   **ê²°ë¡ :** **ì´ìƒ (Abnormal)**\n    *   **ì¶”ì •:** ì„¤ë¹„ê°€ ê°€ë™ë˜ì§€ ì•Šê±°ë‚˜, ë¹„ì •ìƒì ìœ¼ë¡œ ë‚®ì€ ì˜¨ë„ë¥¼ ìœ ì§€í•˜ê³  ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n\n2.  **CT1 (ì „ë¥˜ ì„¼ì„œ)**\n    *   **í˜„ì¬ ê°’:** 0.0A\n    *   **ì •ìƒ ë²”ìœ„:** 20-40A\n    *   **ë¶„ì„:** í˜„ì¬ ê°’ 0.0AëŠ” ì •ìƒ ë²”ìœ„(20-40A)ì— ë¹„í•´ **ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤.**\n    *   **ê²°ë¡ :** **ì´ìƒ (Abnormal)**\n    *   **ì¶”ì •:** ì„¤ë¹„ì— ì „ë ¥ì´ ê³µê¸‰ë˜ì§€ ì•Šê±°ë‚˜, ì‘ë™í•˜ì§€ ì•Šê³  ìˆìŒì„ ê°•ë ¥í•˜ê²Œ ì‹œì‚¬í•©ë‹ˆë‹¤.\n\n3.  **PM2.5 (ë¯¸ì„¸ë¨¼ì§€ ì„¼ì„œ)**\n    *   **í˜„ì¬ ê°’:** 0.1\n    *   **ì •ìƒ ë²”ìœ„:** 0-35\n    *   **ë¶„ì„:** í˜„ì¬ ê°’ 0.1ì€ ì •ìƒ ë²”ìœ„(0-35) **ë‚´ì— ìˆìŠµë‹ˆë‹¤.**\n    *   **ê²°ë¡ :** **ì •ìƒ (Normal)**\n    *   **ì¶”ì •:** ì„¤ë¹„ ì£¼ë³€ì˜ ë¯¸ì„¸ë¨¼ì§€ ë†ë„ëŠ” ì •ìƒì ì¸ ìˆ˜ì¤€ì…ë‹ˆë‹¤.\n\n4.  **ì—´í™”ìƒ (Thermal Imaging)**\n    *   **ìµœê³ :** 1.0Â°C\n    *   **í‰ê· :** 0.1Â°C\n    *   **í¸ì°¨:** 0.08\n    *   **ë¶„ì„:** ì—´í™”ìƒ ë°ì´í„° ìì²´ì— ëŒ€í•œ ëª…í™•í•œ \"ì •ìƒ ë²”ìœ„\"ê°€ ì œì‹œë˜ì§€ ì•Šì•„ ì§ì ‘ì ì¸ ì •ìƒ/ì´ìƒ íŒë‹¨ì€ ì–´ë µìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¤ë¥¸ ì„¼ì„œ(NTC, CT1)ì˜ ì´ìƒ ê°’ì„ ê³ ë ¤í•  ë•Œ, ì´ ë§¤ìš° ë‚®ì€ ì˜¨ë„ ê°’(ìµœê³  1.0Â°C, í‰ê·  0.1Â°C)ì€ ì„¤ë¹„ê°€ **í˜„ì¬ ê°€ë™ë˜ì§€ ì•Šê±°ë‚˜ ë§¤ìš° ì°¨ê°€ìš´ ìƒíƒœ**ì„ì„ ë’·ë°›ì¹¨í•©ë‹ˆë‹¤. í¸ì°¨ê°€ 0.08ë¡œ ë§¤ìš° ë‚®ë‹¤ëŠ” ê²ƒì€ ì„¤ë¹„ ë‚´ë¶€ì˜ ì˜¨ë„ ë¶„í¬ê°€ ë§¤ìš° ê· ì¼í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ì§€ë§Œ, ì´ëŠ” ì„¤ë¹„ê°€ ì‘ë™í•˜ì§€ ì•Šì„ ë•Œ ìì—°ìŠ¤ëŸ¬ìš´ í˜„ìƒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n    *   **ê²°ë¡ :** **ì •ìƒ/ì´ìƒ íŒë‹¨ ë¶ˆê°€ (ë‹¨, ë‹¤ë¥¸ ì„¼ì„œ ì´ìƒê³¼ ì¼ì¹˜í•˜ëŠ” ì €ì˜¨ ìƒíƒœ)**\n\n---\n\n### **ì¢…í•© ë¶„ì„ ë° ì¡°ì¹˜ ê¶Œê³ **\n\nNTC (ì˜¨ë„) ì„¼ì„œì™€ CT1 (ì „ë¥˜) ì„¼ì„œ ëª¨ë‘ ì •ìƒ ë²”ìœ„ë¥¼ í¬ê²Œ ë²—ì–´ë‚˜ **ì´ìƒì„ ë‚˜íƒ€ë‚´ê³  ìˆìŠµë‹ˆë‹¤.** íŠ¹íˆ CT1ì˜ 0.0AëŠ” ì„¤ë¹„ì— ì „ì›ì´ ê³µê¸‰ë˜ì§€ ì•Šê±°ë‚˜, ì•„ì˜ˆ ì‘ë™ì„ ë©ˆì¶˜ ìƒíƒœì„ì„ ê°•ë ¥íˆ ì‹œì‚¬í•©ë‹ˆë‹¤. NTC ì„¼ì„œì˜ 0.3Â°C ë˜í•œ ì„¤ë¹„ê°€ êº¼ì ¸ ìˆê±°ë‚˜, ì£¼ë³€ í™˜ê²½ ì˜¨ë„ì™€ ê±°ì˜ ë™ì¼í•œ ìˆ˜ì¤€ìœ¼ë¡œ ê°€ë™ ì˜¨ë„ê°€ ì•„ë‹˜ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n\n**ê¸´ê¸‰ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.**\n*   **ì „ì› ê³µê¸‰ ì—¬ë¶€ í™•ì¸:** ì„¤ë¹„ì— ì „ì›ì´ ì œëŒ€ë¡œ ê³µê¸‰ë˜ê³  ìˆëŠ”ì§€, ì°¨ë‹¨ê¸°ê°€ ë‚´ë ¤ê°€ ìˆì§€ëŠ” ì•Šì€ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.\n*   **ì„¤ë¹„ ì‘ë™ ìƒíƒœ í™•ì¸:** ì„¤ë¹„ê°€ ì •ìƒì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆëŠ”ì§€, ì˜¤ë¥˜ ì½”ë“œë¥¼ í‘œì‹œí•˜ê³  ìˆì§€ëŠ” ì•Šì€ì§€ ìœ¡ì•ˆìœ¼ë¡œ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.\n*   **ì„¼ì„œ ê³ ì¥ ì—¬ë¶€ í™•ì¸:** ë§Œì•½ ì„¤ë¹„ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆë‹¤ê³  íŒë‹¨ëœë‹¤ë©´, NTC ë° CT1 ì„¼ì„œ ìì²´ì˜ ê³ ì¥ ê°€ëŠ¥ì„±ë„ ë°°ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nPM2.5 ì„¼ì„œê°€ ì •ìƒì¸ ê²ƒì€ ì„¤ë¹„ì˜ ì‘ë™ ì´ìƒê³¼ëŠ” ì§ì ‘ì ì¸ ê´€ë ¨ì´ ì ì€ í™˜ê²½ ìš”ì¸ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\n\n## 2. ì›ì¸ ì¶”ì •\n\nì œê³µëœ ì„¼ì„œ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ìƒ ì›ì¸ì„ 3ê°€ì§€ ì´ë‚´ë¡œ ì¶”ì •í•©ë‹ˆë‹¤.\n\n---\n\n### **ì´ìƒ ì›ì¸ ì¶”ì • (3ê°€ì§€ ì´ë‚´)**\n\n1.  **ì „ì› ê³µê¸‰ ì´ìƒ ë˜ëŠ” ì„¤ë¹„ ì‘ë™ ì •ì§€:**\n    *   **CT1 (ì „ë¥˜ ì„¼ì„œ) ê°’ì´ 0.0A**ì¸ ê²ƒì€ ì„¤ë¹„ì— ì „ë ¥ì´ ì „í˜€ ê³µê¸‰ë˜ì§€ ì•Šê±°ë‚˜, ì„¤ë¹„ ìì²´ê°€ ì™„ì „íˆ ì •ì§€í•˜ì—¬ ì „ë¥˜ë¥¼ ì†Œëª¨í•˜ì§€ ì•Šê³  ìˆìŒì„ ê°•ë ¥í•˜ê²Œ ì‹œì‚¬í•©ë‹ˆë‹¤.\n    *   **NTC (ì˜¨ë„ ì„¼ì„œ) ê°’ì´ 0.3Â°C**ì™€ **ì—´í™”ìƒ ë°ì´í„°ì˜ ë§¤ìš° ë‚®ì€ ì˜¨ë„(ìµœê³  1.0Â°C, í‰ê·  0.1Â°C\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## 10. ì‹œê°í™”","metadata":{}},{"cell_type":"code","source":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=('Part A: LLM ì‘ë‹µ ì‹œê°„', 'Part B: LangGraph'))\n\n# Part A\nmodels = ['Gemini 2.5\\n(API)', 'Gemma-3-4B\\n(Local)', 'Qwen2.5-3B\\n(Local)']\ntimes = [df['gemini_time'].mean(), df['gemma_time'].mean(), df['qwen_time'].mean()]\ncolors = ['#4285F4', '#34A853', '#9B59B6']\nfig.add_trace(go.Bar(x=models, y=times, marker_color=colors,\n    text=[f'{t:.1f}s' for t in times], textposition='outside'), row=1, col=1)\n\n# Part B  \nfig.add_trace(go.Bar(x=['LangGraph\\n(4 Agents)'], y=[elapsed],\n    marker_color='#EA4335', text=[f'{elapsed:.1f}s'], textposition='outside'), row=1, col=2)\n\nfig.update_layout(title='ConveyorGuard LLM ì„±ëŠ¥ ë¹„êµ', height=500, showlegend=False)\nfig.update_yaxes(title_text='ì‘ë‹µ ì‹œê°„ (ì´ˆ)', row=1, col=1)\nfig.show(renderer='iframe')\nfig.write_html(OUTPUT_DIR / 'llm_comparison.html')\nprint('ğŸ“Š llm_comparison.html ì €ì¥')","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:28:59.117417Z","iopub.execute_input":"2026-02-01T16:28:59.117730Z","iopub.status.idle":"2026-02-01T16:28:59.669463Z","shell.execute_reply.started":"2026-02-01T16:28:59.117691Z","shell.execute_reply":"2026-02-01T16:28:59.668738Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"520\"\n    src=\"iframe_figures/figure_15.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}},{"name":"stdout","text":"ğŸ“Š llm_comparison.html ì €ì¥\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 11. ê²°ê³¼ ì €ì¥","metadata":{}},{"cell_type":"code","source":"# CSV\ndf.to_csv(OUTPUT_DIR / 'llm_comparison_results.csv', index=False, encoding='utf-8-sig')\n\n# LangGraph ë¦¬í¬íŠ¸\nwith open(OUTPUT_DIR / 'langgraph_report.md', 'w', encoding='utf-8') as f:\n    f.write(final_report)\n\nprint('âœ… ì €ì¥ ì™„ë£Œ!')\nprint('  - llm_comparison_results.csv')\nprint('  - langgraph_report.md')\nprint('  - llm_comparison.html')","metadata":{"execution":{"iopub.status.busy":"2026-02-01T16:28:59.670371Z","iopub.execute_input":"2026-02-01T16:28:59.670657Z","iopub.status.idle":"2026-02-01T16:28:59.684517Z","shell.execute_reply.started":"2026-02-01T16:28:59.670631Z","shell.execute_reply":"2026-02-01T16:28:59.683865Z"},"trusted":true},"outputs":[{"name":"stdout","text":"âœ… ì €ì¥ ì™„ë£Œ!\n  - llm_comparison_results.csv\n  - langgraph_report.md\n  - llm_comparison.html\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Kaggle ë°ì´í„°ì…‹ ì—…ë¡œë“œ (API)","metadata":{}},{"cell_type":"code","source":"import os, json\n\nos.environ['KAGGLE_USERNAME'] = 'kukass'\nos.environ['KAGGLE_KEY'] = 'KGAT_c973fff8eb3e1ccb19f3e9d683eb17dc'\n\nUPLOAD_DIR = '/kaggle/working/dataset_upload'\nos.makedirs(UPLOAD_DIR, exist_ok=True)\n\noutput_files = ['llm_comparison_results.csv', 'llm_comparison.html']\nfor f in output_files:\n    src = f'/kaggle/working/{f}'\n    dst = f'{UPLOAD_DIR}/{f}'\n    if os.path.exists(src) and not os.path.exists(dst):\n        os.symlink(src, dst)\n\nmeta = {\n    \"title\": \"conveyorguard-llm\",\n    \"id\": \"kukass/conveyorguard-llm\",\n    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n}\nwith open(f'{UPLOAD_DIR}/dataset-metadata.json', 'w') as f:\n    json.dump(meta, f)\n\n!kaggle datasets create -p {UPLOAD_DIR} --dir-mode zip\n\nprint(\"\\nâœ… conveyorguard-llm ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì™„ë£Œ!\")\nprint(\"   â†’ 07_final_comparisonì—ì„œ ì‚¬ìš© ê°€ëŠ¥\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T16:28:59.685459Z","iopub.execute_input":"2026-02-01T16:28:59.685783Z","iopub.status.idle":"2026-02-01T16:29:04.093616Z","shell.execute_reply.started":"2026-02-01T16:28:59.685748Z","shell.execute_reply":"2026-02-01T16:29:04.092818Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Starting upload for file llm_comparison_results.csv\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.20k/3.20k [00:00<00:00, 8.41kB/s]\nUpload successful: llm_comparison_results.csv (3KB)\nStarting upload for file llm_comparison.html\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.36M/4.36M [00:00<00:00, 9.68MB/s]\nUpload successful: llm_comparison.html (4MB)\nYour private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/kukass/conveyorguard-llm\n\nâœ… conveyorguard-llm ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì™„ë£Œ!\n   â†’ 07_final_comparisonì—ì„œ ì‚¬ìš© ê°€ëŠ¥\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## ğŸ“‹ ê²°ê³¼ ìš”ì•½\n\n### Part A: LLM 3ì¢… ë¹„êµ\n\n| ëª¨ë¸ | ìœ í˜• | íŒŒë¼ë¯¸í„° | GPU | íŠ¹ì§• |\n|------|------|----------|-----|------|\n| **Gemini 2.5 Flash** | API | - | - | ê°€ì¥ ë¹ ë¦„, ê³ í’ˆì§ˆ |\n| **Gemma-3-4B** | HuggingFace | 4B | GPU 0 | Google ì˜¤í”ˆì†ŒìŠ¤ |\n| **Qwen2.5-3B** | HuggingFace | 3B | GPU 1 | Alibaba, ë‹¤êµ­ì–´ |\n\n### Part B: LangGraph ë©€í‹° ì—ì´ì „íŠ¸\n\n| ì—ì´ì „íŠ¸ | ì—­í•  |\n|----------|------|\n| **Analyzer** | ì„¼ì„œ ë°ì´í„° ì •ìƒ/ì´ìƒ ë¶„ì„ |\n| **Diagnoser** | ì´ìƒ ì›ì¸ ì¶”ì • |\n| **Advisor** | ìœ ì§€ë³´ìˆ˜ ì¡°ì¹˜ ì¶”ì²œ |\n| **Reviewer** | ì§„ë‹¨ í’ˆì§ˆ ê²€ì¦ (ìµœëŒ€ 2íšŒ ë°˜ë³µ) |\n\n### âš¡ 2GPU ìµœì í™”\n```\nGPU 0: Gemma-3-4B (ë¡œì»¬ ì¶”ë¡ )\nGPU 1: Qwen2.5-3B (ë¡œì»¬ ì¶”ë¡ )\nAPI  : Gemini 2.5 Flash (ë„¤íŠ¸ì›Œí¬)\nâ†’ ê° ëª¨ë¸ì„ ë³„ë„ GPUì— ë°°ì¹˜í•˜ì—¬ ë©”ëª¨ë¦¬ ë¶„ì‚°\n```\n\n### ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ\n```\n00_eda â†’ 01_preprocess â†’ 02_baseline_cnn / 03_ml_baseline â†’ 04_dl_tuning â†’ 05_llm_comparison âœ…\n```","metadata":{}}]}