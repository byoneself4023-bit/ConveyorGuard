{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14685395,"sourceType":"datasetVersion","datasetId":9381506},{"sourceId":14686116,"sourceType":"datasetVersion","datasetId":9381956},{"sourceId":14694023,"sourceType":"datasetVersion","datasetId":9386882},{"sourceId":14695088,"sourceType":"datasetVersion","datasetId":9387553}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"b52216f6-2212-480c-8264-16500e6bab51","cell_type":"markdown","source":"# ConveyorGuard - 06. ì•™ìƒë¸” ëª¨ë¸\n\n**ML 4ì¢… + DL 2ì¢… ì•™ìƒë¸” â†’ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±**\n\n- **Accelerator: GPU T4 x2**\n- ì…ë ¥: `conveyorguard-preprocess`, `conveyorguard-baseline`, `conveyorguard-ml`, `conveyorguard-tuned`\n- ì¶œë ¥: `ensemble_results.csv`, `ensemble_model.pkl`\n\n---\n\n### Pipeline\n> `00_eda` â†’ `01_preprocess` â†’ `02/03`(ë³‘ë ¬) â†’ `04_dl_tuning` â†’ `05_llm` â†’ **`06_ensemble`** â†’ `07_final`\n\n| ì…ë ¥ ë°ì´í„°ì…‹ | ì¶œë ¥ | Accelerator |\n|--------------|------|-------------|\n| 02/03/04 ì¶œë ¥ë¬¼ | `ensemble_results.csv` | GPU T4 x2 |\n\n### ì•™ìƒë¸” ì „ëµ\n```\nML 4ì¢… (XGBoost, LightGBM, RF, CatBoost)\n  + DL 2ì¢… (Baseline CNN, Tuned CNN)\n  â†’ Soft Voting / Weighted Voting / Stacking\n```","metadata":{}},{"id":"06429e53-c6c0-43a4-83ec-061d0a537f03","cell_type":"markdown","source":"## 1. í™˜ê²½ ì„¤ì •","metadata":{}},{"id":"cff600bd-f02c-41d9-a7ff-dbd02deb75da","cell_type":"code","source":"!pip install plotly -q\n\nimport os, sys, time\nimport numpy as np\nimport pandas as pd\nimport joblib\nfrom pathlib import Path\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport plotly.io as pio\npio.renderers.default = 'iframe'\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\n\nprint(f\"Python: {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA: {torch.version.cuda}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"\\nğŸ–¥ï¸ Device: {device}\")\nif torch.cuda.is_available():\n    for i in range(torch.cuda.device_count()):\n        props = torch.cuda.get_device_properties(i)\n        mem = getattr(props, 'total_memory', None) or getattr(props, 'total_mem', 0)\n        print(f\"   GPU {i}: {props.name} ({mem / 1024**3:.1f} GB)\")\n\ntorch.backends.cudnn.benchmark = True\nprint(f\"\\nâœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")\n_notebook_start = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:37:59.697545Z","iopub.execute_input":"2026-02-01T17:37:59.697837Z","iopub.status.idle":"2026-02-01T17:38:11.771138Z","shell.execute_reply.started":"2026-02-01T17:37:59.697817Z","shell.execute_reply":"2026-02-01T17:38:11.770289Z"}},"outputs":[{"name":"stdout","text":"Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\nPyTorch: 2.8.0+cu126\nCUDA: 12.6\n\nğŸ–¥ï¸ Device: cuda\n   GPU 0: Tesla T4 (14.6 GB)\n   GPU 1: Tesla T4 (14.6 GB)\n\nâœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\n","output_type":"stream"}],"execution_count":4},{"id":"80a8fdcd-cf10-4984-b089-de704ef37cb7","cell_type":"code","source":"DATA_DIR = Path('/kaggle/input/conveyorguard-preprocess')\nif not DATA_DIR.exists(): DATA_DIR = Path('/kaggle/input/preprocess')\nBASELINE_DIR = Path('/kaggle/input/conveyorguard-baseline')\nif not BASELINE_DIR.exists(): BASELINE_DIR = Path('/kaggle/input/baseline')\nML_DIR = Path('/kaggle/input/conveyorguard-ml')\nif not ML_DIR.exists(): ML_DIR = Path('/kaggle/input/ml-baseline')\nTUNED_DIR = Path('/kaggle/input/conveyorguard-tuned')\nif not TUNED_DIR.exists(): TUNED_DIR = Path('/kaggle/input/dl-tuning')\nOUTPUT_DIR = Path('/kaggle/working')\n\nfor name, d in [('ì „ì²˜ë¦¬', DATA_DIR), ('Baseline', BASELINE_DIR), ('ML', ML_DIR), ('Tuned', TUNED_DIR)]:\n    print(f\"ğŸ“‚ {name}: {d} ({d.exists()})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:38:18.667063Z","iopub.execute_input":"2026-02-01T17:38:18.667872Z","iopub.status.idle":"2026-02-01T17:38:18.676321Z","shell.execute_reply.started":"2026-02-01T17:38:18.667839Z","shell.execute_reply":"2026-02-01T17:38:18.675614Z"}},"outputs":[{"name":"stdout","text":"ğŸ“‚ ì „ì²˜ë¦¬: /kaggle/input/conveyorguard-preprocess (True)\nğŸ“‚ Baseline: /kaggle/input/conveyorguard-baseline (True)\nğŸ“‚ ML: /kaggle/input/conveyorguard-ml (True)\nğŸ“‚ Tuned: /kaggle/input/conveyorguard-tuned (True)\n","output_type":"stream"}],"execution_count":5},{"id":"c6bcc5a7-e59d-4992-8608-294c538c34d1","cell_type":"markdown","source":"## 2. ë°ì´í„° ë¡œë“œ","metadata":{}},{"id":"3b9819d2-1dde-4cf4-aef5-385aab77a961","cell_type":"code","source":"train_data = joblib.load(DATA_DIR / 'train_data.joblib')\nval_data = joblib.load(DATA_DIR / 'val_data.joblib')\ntest_data = joblib.load(DATA_DIR / 'test_data.joblib')\n\ny_train, y_val, y_test = train_data['labels'], val_data['labels'], test_data['labels']\nprint(f\"Train: {len(y_train):,} / Val: {len(y_val):,} / Test: {len(y_test):,}\")\n\nSTATE_LABELS = ['ì •ìƒ', 'ê²½ë¯¸', 'ì¤‘ê°„', 'ì‹¬ê°']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:38:18.677595Z","iopub.execute_input":"2026-02-01T17:38:18.677864Z","iopub.status.idle":"2026-02-01T17:39:07.202195Z","shell.execute_reply.started":"2026-02-01T17:38:18.677841Z","shell.execute_reply":"2026-02-01T17:39:07.201406Z"}},"outputs":[{"name":"stdout","text":"Train: 7,311 / Val: 1,554 / Test: 1,608\n","output_type":"stream"}],"execution_count":6},{"id":"8275a62c-6c4d-4534-b86e-dc21fb556e14","cell_type":"markdown","source":"## 3. ML ëª¨ë¸ ë¡œë“œ & ì˜ˆì¸¡","metadata":{}},{"id":"b65c1d18-c395-4cf0-a489-c5bc9428a2e8","cell_type":"code","source":"def extract_features(data: dict) -> np.ndarray:\n    \"\"\"\n    ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ë¥¼ MLìš© í”¼ì²˜ë¡œ ë³€í™˜\n    \n    Input:\n        sensors: (N, 30, 8)   â†’ flatten + í†µê³„\n        images: (N, 30, 60, 80) â†’ í†µê³„ê°’\n        external: (N, 30, 3)  â†’ flatten + í†µê³„\n    \n    Output:\n        features: (N, num_features)\n    \"\"\"\n    N = data['sensors'].shape[0]\n    features_list = []\n    \n    # 1. ì„¼ì„œ í”¼ì²˜ (8ê°œ ì„¼ì„œ Ã— 30 íƒ€ì„ìŠ¤í…)\n    sensors = data['sensors']  # (N, 30, 8)\n    \n    # ì„¼ì„œë³„ í†µê³„ (mean, std, max, min, last)\n    sensor_mean = sensors.mean(axis=1)  # (N, 8)\n    sensor_std = sensors.std(axis=1)    # (N, 8)\n    sensor_max = sensors.max(axis=1)    # (N, 8)\n    sensor_min = sensors.min(axis=1)    # (N, 8)\n    sensor_last = sensors[:, -1, :]     # (N, 8) - ë§ˆì§€ë§‰ ê°’\n    sensor_diff = sensors[:, -1, :] - sensors[:, 0, :]  # (N, 8) - ë³€í™”ëŸ‰\n    \n    features_list.extend([sensor_mean, sensor_std, sensor_max, sensor_min, sensor_last, sensor_diff])\n    \n    # 2. ì´ë¯¸ì§€ í”¼ì²˜ (ì—´í™”ìƒ â†’ í†µê³„ê°’)\n    images = data['images']  # (N, 30, 60, 80)\n    \n    # í”„ë ˆì„ë³„ í†µê³„ í›„ ì‹œê³„ì—´ í†µê³„\n    img_frame_mean = images.mean(axis=(2, 3))  # (N, 30)\n    img_frame_max = images.max(axis=(2, 3))    # (N, 30)\n    img_frame_std = images.std(axis=(2, 3))    # (N, 30)\n    \n    # ì‹œê³„ì—´ í†µê³„\n    img_mean = img_frame_mean.mean(axis=1, keepdims=True)  # (N, 1)\n    img_std = img_frame_mean.std(axis=1, keepdims=True)    # (N, 1)\n    img_max = img_frame_max.max(axis=1, keepdims=True)     # (N, 1)\n    img_max_mean = img_frame_max.mean(axis=1, keepdims=True)  # (N, 1)\n    img_last_mean = img_frame_mean[:, -1:]   # (N, 1)\n    img_last_max = img_frame_max[:, -1:]     # (N, 1)\n    img_trend = img_frame_mean[:, -1:] - img_frame_mean[:, 0:1]  # (N, 1) - ì˜¨ë„ ë³€í™” ì¶”ì„¸\n    \n    features_list.extend([img_mean, img_std, img_max, img_max_mean, img_last_mean, img_last_max, img_trend])\n    \n    # 3. ì™¸ë¶€í™˜ê²½ í”¼ì²˜\n    external = data['external']  # (N, 30, 3)\n    \n    ext_mean = external.mean(axis=1)  # (N, 3)\n    ext_std = external.std(axis=1)    # (N, 3)\n    ext_last = external[:, -1, :]     # (N, 3)\n    \n    features_list.extend([ext_mean, ext_std, ext_last])\n    \n    # ëª¨ë“  í”¼ì²˜ ê²°í•©\n    features = np.concatenate(features_list, axis=1)\n    \n    return features\n\nprint(\"âœ… í”¼ì²˜ ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:39:07.203060Z","iopub.execute_input":"2026-02-01T17:39:07.203319Z","iopub.status.idle":"2026-02-01T17:39:07.212285Z","shell.execute_reply.started":"2026-02-01T17:39:07.203299Z","shell.execute_reply":"2026-02-01T17:39:07.211433Z"}},"outputs":[{"name":"stdout","text":"âœ… í”¼ì²˜ ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n","output_type":"stream"}],"execution_count":7},{"id":"6cf906e8-d3a9-42a9-9ffc-657d998cf59c","cell_type":"code","source":"print('ğŸ”§ í”¼ì²˜ ì¶”ì¶œ...')\nX_train = extract_features(train_data)\nX_val = extract_features(val_data)\nX_test = extract_features(test_data)\n\nscaler = joblib.load(ML_DIR / 'ml_scaler.pkl')\nX_train_s, X_val_s, X_test_s = scaler.transform(X_train), scaler.transform(X_val), scaler.transform(X_test)\n\nml_names = ['xgboost', 'lightgbm', 'randomforest', 'catboost']\nml_models, ml_probas_test, ml_probas_val, ml_probas_train = {}, {}, {}, {}\n\nfor name in ml_names:\n    fpath = ML_DIR / f'{name}_model.pkl'\n    if fpath.exists():\n        model = joblib.load(fpath)\n        ml_models[name] = model\n        ml_probas_test[name] = model.predict_proba(X_test_s)\n        ml_probas_val[name] = model.predict_proba(X_val_s)\n        ml_probas_train[name] = model.predict_proba(X_train_s)\n        acc = accuracy_score(y_test, model.predict(X_test_s)) * 100\n        print(f'   âœ… {name}: {acc:.2f}%')\n\nprint(f'\\nâœ… ML {len(ml_models)}ì¢… ë¡œë“œ')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:39:07.213824Z","iopub.execute_input":"2026-02-01T17:39:07.214124Z","iopub.status.idle":"2026-02-01T17:39:20.848887Z","shell.execute_reply.started":"2026-02-01T17:39:07.214080Z","shell.execute_reply":"2026-02-01T17:39:20.848159Z"}},"outputs":[{"name":"stdout","text":"ğŸ”§ í”¼ì²˜ ì¶”ì¶œ...\n   âœ… xgboost: 96.70%\n   âœ… lightgbm: 96.89%\n   âœ… randomforest: 95.58%\n   âœ… catboost: 96.46%\n\nâœ… ML 4ì¢… ë¡œë“œ\n","output_type":"stream"}],"execution_count":8},{"id":"acdb6cc4-f3ac-47fe-9529-461e54e2a5b0","cell_type":"markdown","source":"## 4. DL ëª¨ë¸ ë¡œë“œ & ì˜ˆì¸¡","metadata":{}},{"id":"aeb83d79","cell_type":"code","source":"# ============ 02 Baselineìš© (Dropout2d ì—†ìŒ, norm 1ê°œ) ============\nclass SensorEncoderBaseline(nn.Module):\n    def __init__(self, embed_dim=128):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(8, 64),\n            nn.LayerNorm(64),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, embed_dim),\n            nn.LayerNorm(embed_dim),\n            nn.GELU()\n        )\n    def forward(self, x):\n        return self.mlp(x)\n\nclass ImageEncoderBaseline(nn.Module):\n    def __init__(self, embed_dim=128):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv2d(1, 32, 3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.GELU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.proj = nn.Linear(128, embed_dim)\n    def forward(self, x):\n        B, T, H, W = x.shape\n        x = x.view(B * T, 1, H, W)\n        x = self.cnn(x)\n        x = x.view(B * T, -1)\n        x = self.proj(x)\n        return x.view(B, T, -1)\n\nclass TemporalEncoderBaseline(nn.Module):\n    def __init__(self, embed_dim=128, n_heads=4, n_layers=2):\n        super().__init__()\n        self.pos_embed = nn.Parameter(torch.randn(1, 30, embed_dim) * 0.02)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim, nhead=n_heads, dim_feedforward=embed_dim*4,\n            dropout=0.1, activation='gelu', batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n    def forward(self, x):\n        x = x + self.pos_embed[:, :x.size(1), :]\n        return self.transformer(x)\n\nclass CrossAttentionFusionBaseline(nn.Module):\n    def __init__(self, embed_dim=128, n_heads=4):\n        super().__init__()\n        self.cross_attn = nn.MultiheadAttention(embed_dim, n_heads, batch_first=True)\n        self.norm = nn.LayerNorm(embed_dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim * 4),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(embed_dim * 4, embed_dim)\n        )\n    def forward(self, sensor_feat, image_feat):\n        attn_out, _ = self.cross_attn(sensor_feat, image_feat, image_feat)\n        x = self.norm(sensor_feat + attn_out)\n        x = x + self.ffn(x)\n        return x\n\nclass ConveyorGuardModelBaseline(nn.Module):\n    def __init__(self, embed_dim=128, num_classes=4):\n        super().__init__()\n        self.sensor_encoder = SensorEncoderBaseline(embed_dim)\n        self.image_encoder = ImageEncoderBaseline(embed_dim)\n        self.external_encoder = nn.Sequential(nn.Linear(3, embed_dim), nn.LayerNorm(embed_dim), nn.GELU())\n        self.sensor_temporal = TemporalEncoderBaseline(embed_dim)\n        self.image_temporal = TemporalEncoderBaseline(embed_dim)\n        self.fusion = CrossAttentionFusionBaseline(embed_dim)\n        self.film_gamma = nn.Linear(embed_dim, embed_dim)\n        self.film_beta = nn.Linear(embed_dim, embed_dim)\n        self.classifier = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim), nn.LayerNorm(embed_dim), nn.GELU(),\n            nn.Dropout(0.1), nn.Linear(embed_dim, num_classes)\n        )\n    def forward(self, sensors, images, externals=None):\n        sensor_feat = self.sensor_encoder(sensors)\n        image_feat = self.image_encoder(images)\n        sensor_feat = self.sensor_temporal(sensor_feat)\n        image_feat = self.image_temporal(image_feat)\n        fused = self.fusion(sensor_feat, image_feat)\n        pooled = fused.mean(dim=1)\n        if externals is not None:\n            ext_feat = self.external_encoder(externals)\n            ext_pooled = ext_feat.mean(dim=1)\n            gamma = self.film_gamma(ext_pooled)\n            beta = self.film_beta(ext_pooled)\n            pooled = gamma * pooled + beta\n        return self.classifier(pooled)\n\n# ============ 04 Tunedìš© (ê¸°ì¡´ ê·¸ëŒ€ë¡œ) ============\nclass SensorEncoder(nn.Module):\n    def __init__(self, input_dim=8, embed_dim=128, dropout=0.1):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, 64), nn.LayerNorm(64), nn.GELU(), nn.Dropout(dropout),\n            nn.Linear(64, embed_dim), nn.LayerNorm(embed_dim), nn.GELU()\n        )\n    def forward(self, x):\n        return self.mlp(x)\n\nclass ImageEncoder(nn.Module):\n    def __init__(self, embed_dim=128, dropout=0.1):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv2d(1, 32, 3, padding=1), nn.BatchNorm2d(32), nn.GELU(), nn.MaxPool2d(2), nn.Dropout2d(dropout),\n            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.GELU(), nn.MaxPool2d(2), nn.Dropout2d(dropout),\n            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.GELU(), nn.AdaptiveAvgPool2d(1)\n        )\n        self.proj = nn.Linear(128, embed_dim)\n    def forward(self, x):\n        B, T, H, W = x.shape\n        x = x.view(B * T, 1, H, W)\n        x = self.cnn(x)\n        x = x.view(B * T, -1)\n        x = self.proj(x)\n        return x.view(B, T, -1)\n\nclass TemporalEncoder(nn.Module):\n    def __init__(self, embed_dim=128, num_heads=4, num_layers=2, dropout=0.1, seq_len=30):\n        super().__init__()\n        self.pos_embed = nn.Parameter(torch.randn(1, seq_len, embed_dim) * 0.02)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim*4,\n            dropout=dropout, activation='gelu', batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n    def forward(self, x):\n        x = x + self.pos_embed[:, :x.size(1), :]\n        return self.transformer(x)\n\nclass CrossAttentionFusion(nn.Module):\n    def __init__(self, embed_dim=128, num_heads=4, dropout=0.1):\n        super().__init__()\n        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim*4), nn.GELU(), nn.Dropout(dropout),\n            nn.Linear(embed_dim*4, embed_dim), nn.Dropout(dropout)\n        )\n    def forward(self, sensor_feat, image_feat):\n        attn_out, _ = self.cross_attn(sensor_feat, image_feat, image_feat)\n        x = self.norm1(sensor_feat + attn_out)\n        x = self.norm2(x + self.ffn(x))\n        return x\n\nclass ConveyorGuardModel(nn.Module):\n    def __init__(self, embed_dim=128, num_heads=4, num_layers=2, dropout=0.1, num_classes=4, sensor_seq_len=30, image_seq_len=10):\n        super().__init__()\n        self.sensor_encoder = SensorEncoder(8, embed_dim, dropout)\n        self.image_encoder = ImageEncoder(embed_dim, dropout)\n        self.external_encoder = nn.Sequential(nn.Linear(3, embed_dim), nn.LayerNorm(embed_dim), nn.GELU())\n        self.sensor_temporal = TemporalEncoder(embed_dim, num_heads, num_layers, dropout, sensor_seq_len)\n        self.image_temporal = TemporalEncoder(embed_dim, num_heads, num_layers, dropout, image_seq_len)\n        self.fusion = CrossAttentionFusion(embed_dim, num_heads, dropout)\n        self.film_gamma = nn.Linear(embed_dim, embed_dim)\n        self.film_beta = nn.Linear(embed_dim, embed_dim)\n        self.classifier = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim), nn.LayerNorm(embed_dim), nn.GELU(),\n            nn.Dropout(dropout), nn.Linear(embed_dim, num_classes)\n        )\n    def forward(self, sensors, images, externals=None):\n        sensor_feat = self.sensor_encoder(sensors)\n        image_feat = self.image_encoder(images)\n        sensor_feat = self.sensor_temporal(sensor_feat)\n        image_feat = self.image_temporal(image_feat)\n        fused = self.fusion(sensor_feat, image_feat)\n        pooled = fused.mean(dim=1)\n        if externals is not None:\n            ext_feat = self.external_encoder(externals)\n            ext_pooled = ext_feat.mean(dim=1)\n            pooled = self.film_gamma(ext_pooled) * pooled + self.film_beta(ext_pooled)\n        return self.classifier(pooled)\n\nprint(\"âœ… ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ (Baseline + Tuned)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:50:48.796747Z","iopub.execute_input":"2026-02-01T17:50:48.797145Z","iopub.status.idle":"2026-02-01T17:50:48.824589Z","shell.execute_reply.started":"2026-02-01T17:50:48.797108Z","shell.execute_reply":"2026-02-01T17:50:48.823730Z"}},"outputs":[{"name":"stdout","text":"âœ… ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ (Baseline + Tuned)\n","output_type":"stream"}],"execution_count":12},{"id":"430bc03f-13ab-45ff-ab83-fca1bbb55563","cell_type":"code","source":"class ConveyorDataset(Dataset):\n    def __init__(self, data: dict):\n        self.sensors = torch.FloatTensor(data['sensors'])\n        self.images = torch.FloatTensor(data['images'])\n        self.externals = torch.FloatTensor(data['external'])  # í‚¤ ì´ë¦„ í†µì¼: 'external'\n        self.labels = torch.LongTensor(data['labels'])\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return {\n            'sensors': self.sensors[idx],\n            'images': self.images[idx],\n            'externals': self.externals[idx],\n            'label': self.labels[idx]\n        }\n\n# Dataset ìƒì„±\ntrain_dataset = ConveyorDataset(train_data)\nval_dataset = ConveyorDataset(val_data)\n\nprint(f\"âœ… Dataset ìƒì„± ì™„ë£Œ\")\nprint(f\"   Train: {len(train_dataset)}ê°œ\")\nprint(f\"   Val: {len(val_dataset)}ê°œ\")\n\nclass ConveyorDatasetOptimized(Dataset):\n    \"\"\"\n    ìµœì í™”ëœ Dataset\n    - ì´ë¯¸ì§€: 30í”„ë ˆì„ â†’ 10í”„ë ˆì„ ì„œë¸Œìƒ˜í”Œë§ (3ë°° ë¹¨ë¼ì§)\n    \"\"\"\n    def __init__(self, data: dict, image_frames: int = 10):\n        self.sensors = torch.FloatTensor(data['sensors'])\n        \n        # âš¡ ì´ë¯¸ì§€ ì„œë¸Œìƒ˜í”Œë§ (30 â†’ 10 í”„ë ˆì„)\n        images = data['images']\n        indices = np.linspace(0, images.shape[1]-1, image_frames, dtype=int)\n        self.images = torch.FloatTensor(images[:, indices, :, :])\n        \n        self.externals = torch.FloatTensor(data['external'])\n        self.labels = torch.LongTensor(data['labels'])\n        \n        self.image_frames = image_frames\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return {\n            'sensors': self.sensors[idx],\n            'images': self.images[idx],\n            'externals': self.externals[idx],\n            'label': self.labels[idx]\n        }\n\n# Dataset ìƒì„±\nIMAGE_FRAMES = 10  # 30 â†’ 10 (ìµœì í™”)\n\ntrain_dataset = ConveyorDatasetOptimized(train_data, image_frames=IMAGE_FRAMES)\nval_dataset = ConveyorDatasetOptimized(val_data, image_frames=IMAGE_FRAMES)\ntest_dataset = ConveyorDatasetOptimized(test_data, image_frames=IMAGE_FRAMES)\n\nprint(f\"âœ… Dataset ìƒì„± ì™„ë£Œ (ì´ë¯¸ì§€ {IMAGE_FRAMES}í”„ë ˆì„)\")\nprint(f\"   Train: {len(train_dataset):,}ê°œ\")\nprint(f\"   Val: {len(val_dataset):,}ê°œ\")\nprint(f\"   Test: {len(test_dataset):,}ê°œ\")\nprint(f\"   ì´ë¯¸ì§€ shape: {train_dataset.images.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:50:53.761478Z","iopub.execute_input":"2026-02-01T17:50:53.762260Z","iopub.status.idle":"2026-02-01T17:50:54.351983Z","shell.execute_reply.started":"2026-02-01T17:50:53.762219Z","shell.execute_reply":"2026-02-01T17:50:54.351231Z"}},"outputs":[{"name":"stdout","text":"âœ… Dataset ìƒì„± ì™„ë£Œ\n   Train: 7311ê°œ\n   Val: 1554ê°œ\nâœ… Dataset ìƒì„± ì™„ë£Œ (ì´ë¯¸ì§€ 10í”„ë ˆì„)\n   Train: 7,311ê°œ\n   Val: 1,554ê°œ\n   Test: 1,608ê°œ\n   ì´ë¯¸ì§€ shape: torch.Size([7311, 10, 60, 80])\n","output_type":"stream"}],"execution_count":13},{"id":"921d668e","cell_type":"code","source":"BATCH_SIZE = 64\n\nbaseline_ckpt = torch.load(BASELINE_DIR / 'baseline_cnn_model.pt', map_location=device)\nbaseline_model = ConveyorGuardModelBaseline(embed_dim=128, num_classes=4).to(device)\nbaseline_model.load_state_dict(baseline_ckpt['model_state_dict'])\nbaseline_model.eval()\nprint(f'âœ… Baseline (Best Acc: {baseline_ckpt.get(\"best_acc\", \"N/A\")}) [image_seq_len=30]')\n\ntuned_ckpt = torch.load(TUNED_DIR / 'tuned_model.pt', map_location=device)\ntc = tuned_ckpt['model_config']\ntuned_model = ConveyorGuardModel(\n    embed_dim=tc['embed_dim'], num_heads=tc.get('num_heads',4),\n    num_layers=tc.get('num_layers',1), dropout=tc.get('dropout',0.1),\n    num_classes=4, image_seq_len=tc.get('image_seq_len',10)\n).to(device)\ntuned_model.load_state_dict(tuned_ckpt['model_state_dict'])\ntuned_model.eval()\nprint(f'âœ… Tuned (Val Acc: {tuned_ckpt.get(\"best_val_acc\", \"N/A\")}) [image_seq_len={tc.get(\"image_seq_len\",10)}]')\n\ndef get_dl_probas(model, data, image_frames=None):\n    ds = ConveyorDatasetOptimized(data, image_frames) if image_frames else ConveyorDataset(data)\n    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n    all_p = []\n    with torch.no_grad():\n        for batch in loader:\n            s = batch['sensors'].to(device, non_blocking=True)\n            im = batch['images'].to(device, non_blocking=True)\n            ex = batch['externals'].to(device, non_blocking=True)\n            with autocast():\n                logits = model(s, im, ex)\n            all_p.append(F.softmax(logits, dim=1).cpu().numpy())\n    return np.concatenate(all_p)\n\nimg_f = tc.get('image_seq_len', 10)\ndl_probas, dl_probas_val, dl_probas_train = {}, {}, {}\nfor nm, mdl, ifr in [('baseline', baseline_model, None), ('tuned', tuned_model, img_f)]:\n    dl_probas[nm] = get_dl_probas(mdl, test_data, ifr)\n    dl_probas_val[nm] = get_dl_probas(mdl, val_data, ifr)\n    dl_probas_train[nm] = get_dl_probas(mdl, train_data, ifr)\n    print(f'   âœ… {nm}: {accuracy_score(y_test, dl_probas[nm].argmax(1))*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:51:38.974825Z","iopub.execute_input":"2026-02-01T17:51:38.975172Z","iopub.status.idle":"2026-02-01T17:52:03.015442Z","shell.execute_reply.started":"2026-02-01T17:51:38.975147Z","shell.execute_reply":"2026-02-01T17:52:03.014593Z"}},"outputs":[{"name":"stdout","text":"âœ… Baseline (Best Acc: 93.24324324324324) [image_seq_len=30]\nâœ… Tuned (Val Acc: 90.34749034749035) [image_seq_len=10]\n   âœ… baseline: 92.72%\n   âœ… tuned: 87.75%\n","output_type":"stream"}],"execution_count":14},{"id":"ce606c91-5532-471d-b190-62ef63a03c6b","cell_type":"markdown","source":"## 5. ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í™•ì¸","metadata":{}},{"id":"fb41c200-0f26-4603-8391-2db7bb8fb8ca","cell_type":"code","source":"all_probas_test = {**ml_probas_test, **dl_probas}\nall_probas_val = {**ml_probas_val, **dl_probas_val}\n\nindividual_results = []\nfor name, proba in all_probas_test.items():\n    preds = proba.argmax(1)\n    val_preds = all_probas_val[name].argmax(1)\n    individual_results.append({\n        'Model': name, 'Val_Acc': accuracy_score(y_val, val_preds)*100,\n        'Test_Acc': accuracy_score(y_test, preds)*100,\n        'Test_F1': f1_score(y_test, preds, average='weighted')*100,\n    })\n\nind_df = pd.DataFrame(individual_results).sort_values('Test_Acc', ascending=False)\nprint('ğŸ“Š ê°œë³„ ëª¨ë¸ ì„±ëŠ¥:')\nprint(ind_df.to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:52:24.309470Z","iopub.execute_input":"2026-02-01T17:52:24.309818Z","iopub.status.idle":"2026-02-01T17:52:24.354223Z","shell.execute_reply.started":"2026-02-01T17:52:24.309786Z","shell.execute_reply":"2026-02-01T17:52:24.353506Z"}},"outputs":[{"name":"stdout","text":"ğŸ“Š ê°œë³„ ëª¨ë¸ ì„±ëŠ¥:\n       Model   Val_Acc  Test_Acc   Test_F1\n    lightgbm 96.975547 96.890547 96.890147\n     xgboost 97.039897 96.703980 96.704233\n    catboost 96.589447 96.455224 96.456080\nrandomforest 96.525097 95.584577 95.631784\n    baseline 93.243243 92.723881 92.799636\n       tuned 88.095238 87.748756 88.129900\n","output_type":"stream"}],"execution_count":15},{"id":"7d731bd0-1ec1-435f-9e80-6384e92f7de6","cell_type":"markdown","source":"## 6. ì•™ìƒë¸” ì „ëµ","metadata":{}},{"id":"b34a85dc-a82f-40aa-9223-c6c9b4c4932e","cell_type":"markdown","source":"### 6.1 Soft Voting (ê· ë“± ê°€ì¤‘)","metadata":{}},{"id":"2d182ae0-6ae3-47d9-9174-26b64f2d57eb","cell_type":"code","source":"all_test_probas = list(all_probas_test.values())\nsoft_avg = np.mean(all_test_probas, axis=0)\nsoft_preds = soft_avg.argmax(1)\nsoft_acc = accuracy_score(y_test, soft_preds)*100\nsoft_f1 = f1_score(y_test, soft_preds, average='weighted')*100\nprint(f'ğŸ¯ Soft Voting: Acc={soft_acc:.2f}%, F1={soft_f1:.2f}%')\nprint(classification_report(y_test, soft_preds, target_names=STATE_LABELS))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:52:24.355385Z","iopub.execute_input":"2026-02-01T17:52:24.355627Z","iopub.status.idle":"2026-02-01T17:52:24.371939Z","shell.execute_reply.started":"2026-02-01T17:52:24.355606Z","shell.execute_reply":"2026-02-01T17:52:24.371393Z"}},"outputs":[{"name":"stdout","text":"ğŸ¯ Soft Voting: Acc=96.64%, F1=96.64%\n              precision    recall  f1-score   support\n\n          ì •ìƒ       0.99      0.99      0.99       788\n          ê²½ë¯¸       0.93      0.94      0.93       371\n          ì¤‘ê°„       0.96      0.94      0.95       361\n          ì‹¬ê°       0.97      0.98      0.97        88\n\n    accuracy                           0.97      1608\n   macro avg       0.96      0.96      0.96      1608\nweighted avg       0.97      0.97      0.97      1608\n\n","output_type":"stream"}],"execution_count":16},{"id":"c6f5c6dd-0580-4b3f-95b5-ceeb3351c8e8","cell_type":"markdown","source":"### 6.2 Weighted Voting (Val Acc ê¸°ë°˜)","metadata":{}},{"id":"9766fca1-6496-47a5-92c5-efbdf5ba9f73","cell_type":"code","source":"weights = np.array([accuracy_score(y_val, all_probas_val[n].argmax(1)) for n in all_probas_test])\nweights = weights / weights.sum()\nfor n, w in zip(all_probas_test, weights): print(f'   {n}: {w:.4f}')\n\nweighted_avg = np.average(all_test_probas, axis=0, weights=weights)\nweighted_preds = weighted_avg.argmax(1)\nweighted_acc = accuracy_score(y_test, weighted_preds)*100\nweighted_f1 = f1_score(y_test, weighted_preds, average='weighted')*100\nprint(f'\\nğŸ¯ Weighted: Acc={weighted_acc:.2f}%, F1={weighted_f1:.2f}%')\nprint(classification_report(y_test, weighted_preds, target_names=STATE_LABELS))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:52:24.372891Z","iopub.execute_input":"2026-02-01T17:52:24.373179Z","iopub.status.idle":"2026-02-01T17:52:24.392229Z","shell.execute_reply.started":"2026-02-01T17:52:24.373150Z","shell.execute_reply":"2026-02-01T17:52:24.391538Z"}},"outputs":[{"name":"stdout","text":"   xgboost: 0.1707\n   lightgbm: 0.1706\n   randomforest: 0.1698\n   catboost: 0.1699\n   baseline: 0.1640\n   tuned: 0.1550\n\nğŸ¯ Weighted: Acc=96.70%, F1=96.71%\n              precision    recall  f1-score   support\n\n          ì •ìƒ       0.99      0.99      0.99       788\n          ê²½ë¯¸       0.93      0.94      0.94       371\n          ì¤‘ê°„       0.96      0.94      0.95       361\n          ì‹¬ê°       0.97      0.98      0.97        88\n\n    accuracy                           0.97      1608\n   macro avg       0.96      0.96      0.96      1608\nweighted avg       0.97      0.97      0.97      1608\n\n","output_type":"stream"}],"execution_count":17},{"id":"72097c1c-bae3-4ec5-96b4-d197194e6815","cell_type":"markdown","source":"### 6.3 Stacking (Meta-Learner)","metadata":{}},{"id":"b1eaa703-0851-4842-9eba-8e0917b07dbd","cell_type":"code","source":"all_train_probas = {**ml_probas_train, **dl_probas_train}\nmeta_train = np.hstack([all_train_probas[n] for n in all_probas_test])\nmeta_val = np.hstack([all_probas_val[n] for n in all_probas_test])\nmeta_test = np.hstack([all_probas_test[n] for n in all_probas_test])\nprint(f'meta-features: {meta_train.shape[1]} ({len(all_probas_test)}ëª¨ë¸Ã—4í´ë˜ìŠ¤)')\n\nmeta_learner = LogisticRegression(max_iter=1000, random_state=42)\nmeta_learner.fit(meta_train, y_train)\n\nstack_preds = meta_learner.predict(meta_test)\nstack_acc = accuracy_score(y_test, stack_preds)*100\nstack_f1 = f1_score(y_test, stack_preds, average='weighted')*100\nmeta_val_acc = accuracy_score(y_val, meta_learner.predict(meta_val))*100\nprint(f'\\nğŸ¯ Stacking: Val={meta_val_acc:.2f}%, Test={stack_acc:.2f}%, F1={stack_f1:.2f}%')\nprint(classification_report(y_test, stack_preds, target_names=STATE_LABELS))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:52:24.393637Z","iopub.execute_input":"2026-02-01T17:52:24.393861Z","iopub.status.idle":"2026-02-01T17:52:24.448330Z","shell.execute_reply.started":"2026-02-01T17:52:24.393844Z","shell.execute_reply":"2026-02-01T17:52:24.447770Z"}},"outputs":[{"name":"stdout","text":"meta-features: 24 (6ëª¨ë¸Ã—4í´ë˜ìŠ¤)\n\nğŸ¯ Stacking: Val=96.91%, Test=96.89%, F1=96.89%\n              precision    recall  f1-score   support\n\n          ì •ìƒ       0.99      0.99      0.99       788\n          ê²½ë¯¸       0.93      0.94      0.94       371\n          ì¤‘ê°„       0.96      0.94      0.95       361\n          ì‹¬ê°       0.98      0.98      0.98        88\n\n    accuracy                           0.97      1608\n   macro avg       0.96      0.96      0.96      1608\nweighted avg       0.97      0.97      0.97      1608\n\n","output_type":"stream"}],"execution_count":18},{"id":"dda604b9-4c2f-43c0-931c-c3e9e98a27f4","cell_type":"markdown","source":"## 7. ê²°ê³¼ ë¹„êµ","metadata":{}},{"id":"ffbc8c28-971e-4ac7-8483-7c6a894f3684","cell_type":"code","source":"ensemble_results = individual_results.copy()\nensemble_results.append({'Model':'Soft Voting','Val_Acc':None,'Test_Acc':soft_acc,'Test_F1':soft_f1})\nensemble_results.append({'Model':'Weighted Voting','Val_Acc':None,'Test_Acc':weighted_acc,'Test_F1':weighted_f1})\nensemble_results.append({'Model':'Stacking','Val_Acc':meta_val_acc,'Test_Acc':stack_acc,'Test_F1':stack_f1})\nresults_df = pd.DataFrame(ensemble_results).sort_values('Test_Acc', ascending=False)\nprint(results_df.to_string(index=False))\n\ncolors = ['#EF4444' if any(k in m for k in ['Voting','Stacking']) else '#3B82F6' for m in results_df['Model']]\nfig = go.Figure(go.Bar(x=results_df['Model'], y=results_df['Test_Acc'],\n    marker_color=colors, text=results_df['Test_Acc'].round(2), textposition='outside'))\nfig.update_layout(title='ëª¨ë¸ë³„ Test Accuracy (ê°œë³„ vs ì•™ìƒë¸”)',\n    xaxis_title='Model', yaxis_title='Acc (%)', height=500, width=900,\n    yaxis=dict(range=[max(0,results_df['Test_Acc'].min()-5), 100]))\nfig.show(renderer='iframe')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:52:24.448898Z","iopub.execute_input":"2026-02-01T17:52:24.449108Z","iopub.status.idle":"2026-02-01T17:52:24.634168Z","shell.execute_reply.started":"2026-02-01T17:52:24.449075Z","shell.execute_reply":"2026-02-01T17:52:24.633419Z"}},"outputs":[{"name":"stdout","text":"          Model   Val_Acc  Test_Acc   Test_F1\n       lightgbm 96.975547 96.890547 96.890147\n       Stacking 96.911197 96.890547 96.890923\n        xgboost 97.039897 96.703980 96.704233\nWeighted Voting       NaN 96.703980 96.707115\n    Soft Voting       NaN 96.641791 96.644509\n       catboost 96.589447 96.455224 96.456080\n   randomforest 96.525097 95.584577 95.631784\n       baseline 93.243243 92.723881 92.799636\n          tuned 88.095238 87.748756 88.129900\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"920px\"\n    height=\"520\"\n    src=\"iframe_figures/figure_19.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}}],"execution_count":19},{"id":"fac28500-33e4-4b64-8692-00028c93abfe","cell_type":"code","source":"best_row = results_df.iloc[0]\nbm = best_row['Model']\nif 'Stacking' in bm: bp = stack_preds\nelif 'Weighted' in bm: bp = weighted_preds\nelif 'Soft' in bm: bp = soft_preds\nelse: bp = all_probas_test[bm].argmax(1)\n\ncm = confusion_matrix(y_test, bp)\nfig = ff.create_annotated_heatmap(z=cm, x=STATE_LABELS, y=STATE_LABELS,\n    colorscale='Blues', showscale=True)\nfig.update_layout(title=f'Confusion Matrix â€” {bm} ({best_row[\"Test_Acc\"]:.2f}%)',\n    xaxis_title='Predicted', yaxis_title='Actual', width=600, height=500)\nfig.update_yaxes(autorange='reversed')\nfig.show(renderer='iframe')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:52:24.635169Z","iopub.execute_input":"2026-02-01T17:52:24.635556Z","iopub.status.idle":"2026-02-01T17:52:24.722371Z","shell.execute_reply.started":"2026-02-01T17:52:24.635523Z","shell.execute_reply":"2026-02-01T17:52:24.721749Z"}},"outputs":[{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"620px\"\n    height=\"520\"\n    src=\"iframe_figures/figure_20.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}}],"execution_count":20},{"id":"6094621f-44ad-44b8-af84-1b467e51d6ef","cell_type":"markdown","source":"## 8. ëª¨ë¸ ì €ì¥","metadata":{}},{"id":"fa86b158-6bec-4073-bb49-8bf80eb03b90","cell_type":"code","source":"results_df.to_csv(OUTPUT_DIR / 'ensemble_results.csv', index=False, encoding='utf-8-sig')\njoblib.dump(meta_learner, OUTPUT_DIR / 'ensemble_model.pkl', compress=3)\nprint(f'âœ… ì €ì¥ ì™„ë£Œ')\nprint(f'ğŸ† Best: {best_row[\"Model\"]} â†’ {best_row[\"Test_Acc\"]:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:52:24.723137Z","iopub.execute_input":"2026-02-01T17:52:24.723339Z","iopub.status.idle":"2026-02-01T17:52:24.734321Z","shell.execute_reply.started":"2026-02-01T17:52:24.723320Z","shell.execute_reply":"2026-02-01T17:52:24.733710Z"}},"outputs":[{"name":"stdout","text":"âœ… ì €ì¥ ì™„ë£Œ\nğŸ† Best: lightgbm â†’ 96.89%\n","output_type":"stream"}],"execution_count":21},{"id":"e763a1ce-a51b-4fea-a73e-dc7a3c44defc","cell_type":"code","source":"_total_time = time.time() - _notebook_start\nprint(f\"\\nâ±ï¸ ì´ ì‹¤í–‰ ì‹œê°„: {_total_time/60:.1f}ë¶„ ({_total_time:.0f}ì´ˆ)\")\nprint('\\nâœ… 06_ensemble ì™„ë£Œ! ë‹¤ìŒ: 07_final_comparison.ipynb')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:52:24.735223Z","iopub.execute_input":"2026-02-01T17:52:24.735470Z","iopub.status.idle":"2026-02-01T17:52:24.741661Z","shell.execute_reply.started":"2026-02-01T17:52:24.735451Z","shell.execute_reply":"2026-02-01T17:52:24.740980Z"}},"outputs":[{"name":"stdout","text":"\nâ±ï¸ ì´ ì‹¤í–‰ ì‹œê°„: 14.2ë¶„ (853ì´ˆ)\n\nâœ… 06_ensemble ì™„ë£Œ! ë‹¤ìŒ: 07_final_comparison.ipynb\n","output_type":"stream"}],"execution_count":22},{"id":"951023d9-709c-427f-bbb2-83a202dc82eb","cell_type":"markdown","source":"## Kaggle ë°ì´í„°ì…‹ ì—…ë¡œë“œ (API)","metadata":{}},{"id":"2f37fd2b-4a58-4354-8991-b0f4bfa864d1","cell_type":"code","source":"import os, json\n\nos.environ['KAGGLE_USERNAME'] = 'kukass'\nos.environ['KAGGLE_KEY'] = 'KGAT_c973fff8eb3e1ccb19f3e9d683eb17dc'\n\nUPLOAD_DIR = '/kaggle/working/dataset_upload'\nos.makedirs(UPLOAD_DIR, exist_ok=True)\n\noutput_files = ['ensemble_results.csv', 'ensemble_model.pkl']\nfor f in output_files:\n    src = f'/kaggle/working/{f}'\n    dst = f'{UPLOAD_DIR}/{f}'\n    if os.path.exists(src) and not os.path.exists(dst):\n        os.symlink(src, dst)\n\nmeta = {\n    \"title\": \"conveyorguard-ensemble\",\n    \"id\": \"kukass/conveyorguard-ensemble\",\n    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n}\nwith open(f'{UPLOAD_DIR}/dataset-metadata.json', 'w') as f:\n    json.dump(meta, f)\n\n!kaggle datasets create -p {UPLOAD_DIR} --dir-mode zip\n\nprint(\"\\nâœ… conveyorguard-ensemble ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì™„ë£Œ!\")\nprint(\"   â†’ 07_final_comparisonì—ì„œ ì‚¬ìš© ê°€ëŠ¥\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T17:52:24.743040Z","iopub.execute_input":"2026-02-01T17:52:24.743419Z","iopub.status.idle":"2026-02-01T17:52:29.458213Z","shell.execute_reply.started":"2026-02-01T17:52:24.743400Z","shell.execute_reply":"2026-02-01T17:52:29.457447Z"}},"outputs":[{"name":"stdout","text":"Starting upload for file ensemble_results.csv\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 574/574 [00:00<00:00, 1.50kB/s]\nUpload successful: ensemble_results.csv (574B)\nStarting upload for file ensemble_model.pkl\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.38k/1.38k [00:00<00:00, 3.82kB/s]\nUpload successful: ensemble_model.pkl (1KB)\nYour private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/kukass/conveyorguard-ensemble\n\nâœ… conveyorguard-ensemble ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì™„ë£Œ!\n   â†’ 07_final_comparisonì—ì„œ ì‚¬ìš© ê°€ëŠ¥\n","output_type":"stream"}],"execution_count":23},{"id":"63a9bf0b-d0d7-4242-a192-3cda1772e504","cell_type":"markdown","source":"---\n\n## ğŸ“‹ ê²°ê³¼ ìš”ì•½\n\n| ì „ëµ | ì„¤ëª… |\n|----|----|\n| **Soft Voting** | ê· ë“± ê°€ì¤‘ í‰ê·  |\n| **Weighted Voting** | Val Acc ë¹„ë¡€ ê°€ì¤‘ |\n| **Stacking** | Meta-learner (LR) |\n\n### ë‹¤ìŒ ë‹¨ê³„\n- `07_final_comparison.ipynb`: ì „ì²´ ëª¨ë¸ ë¹„êµ + ìµœì¢… ê²°ë¡ ","metadata":{}}]}