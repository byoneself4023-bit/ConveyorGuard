# ConveyorGuard LLM 진단 비교 + LangGraph 멀티 에이전트 결과

## 1. 목표

> **3종 LLM 비교 + LangGraph 멀티 에이전트로 체계적인 설비 진단 시스템 구축**

### 핵심 질문
- 어떤 LLM이 설비 진단에 가장 적합한가?
- 멀티 에이전트 워크플로우가 단순 LLM 호출보다 나은가?
- 실제 현장 적용 가능한 진단 리포트를 생성할 수 있는가?

---

## 2. LLM 3종 비교 구성

### 비교 대상

| 모델 | 유형 | 파라미터 | GPU | 특징 |
|------|------|----------|-----|------|
| **Gemini 2.5 Flash** | API | 비공개 | - | Google 최신, 고품질 |
| **Gemma-3-4B** | 로컬 | 4B | GPU 0 | Google 오픈소스 |
| **Qwen2.5-3B** | 로컬 | 3B | GPU 1 | Alibaba, 다국어 |

### 비교 축

```
API vs 로컬       → 속도, 비용, 프라이버시 트레이드오프
Google vs Alibaba → 다양한 벤더 비교
대형 vs 소형      → T4 16GB 제약 내 최적 모델 탐색
```

### T4 GPU 제약

| 모델 | VRAM 필요 | T4 16GB | 선택 |
|------|-----------|---------|------|
| Gemma-3-12B | ~24GB | ❌ | - |
| Qwen2.5-7B | ~14GB | 빠듯 | - |
| Gemma-3-4B | ~8GB | ✅ | 채택 |
| Qwen2.5-3B | ~6GB | ✅ | 채택 |

**4B + 3B 조합 = T4 x2에 각각 배치 가능한 최대 크기**

---

## 3. Part A: LLM 3종 비교 결과

### 테스트 설정

- 테스트 샘플: 4개 (정상, 경미, 중간, 심각 각 1개)
- 프롬프트: 센서값 + 열화상 데이터 → 100자 이내 진단

### 응답 시간 비교

| 모델 | 평균 응답 시간 | 순위 |
|------|---------------|------|
| **Gemini 2.5 Flash** | **7.99초** | 🥇 |
| Qwen2.5-3B | 8.12초 | 🥈 |
| Gemma-3-4B | 14.65초 | 🥉 |

### 응답 품질 비교 (심각 케이스)

**입력 데이터:**
```
NTC: 0.3°C (정상: 40-60°C) → 이상
CT1: 0.0A (정상: 20-40A) → 이상
열화상: 최고 1.0°C, 평균 0.1°C → 설비 미가동 추정
```

**각 모델 응답:**

| 모델 | 응답 | 평가 |
|------|------|------|
| **Gemini** | "장비 미가동 또는 전원 이상 추정. CT 전류 0.0A 및 낮은 온도가 지표. 전원 및 장비 작동 상태를 점검하고 필요 시 전원 공급 조치." | ✅ 정확, 명확 |
| **Gemma** | N/A | ❌ 응답 실패 |
| **Qwen** | "장비 온도 제어 불량, CT1 과부하..." (반복 문구 포함) | △ 응답은 있지만 품질 낮음 |

### Part A 결론

| 지표 | 1위 |
|------|-----|
| 속도 | Gemini 2.5 Flash |
| 품질 | Gemini 2.5 Flash |
| 안정성 | Gemini 2.5 Flash |

**Gemini 2.5 Flash가 속도와 품질 모두 최고!**

---

## 4. Part B: LangGraph 멀티 에이전트

### 왜 LangGraph인가?

**일반 LLM 호출:**
```
센서 데이터 → LLM → "이상입니다" (끝)
```
→ 한 번에 모든 걸 처리 → 답변 품질 들쭉날쭉

**LangGraph 방식:**
```
센서 데이터
    ↓
[Analyzer] → "NTC 0.3°C, CT1 0.0A 이상"
    ↓
[Diagnoser] → "전원 공급 문제 추정"
    ↓
[Advisor] → "1. 차단기 확인 2. 전원 점검"
    ↓
[Reviewer] → "분석 부족" → REVISE → 다시 진단
    ↓
[Reviewer] → "APPROVE" → 최종 리포트
```
→ 역할 분담 + 검토 루프 → 체계적인 결과

### 에이전트 구성

| 에이전트 | 역할 | 출력 |
|----------|------|------|
| **Analyzer** | 센서 데이터 정상/이상 분석 | 각 센서별 판정 |
| **Diagnoser** | 이상 원인 추정 (3가지 이내) | 원인 목록 |
| **Advisor** | 유지보수 조치 추천 | 즉시/예방/점검 조치 |
| **Reviewer** | 진단 품질 검증 | APPROVE 또는 REVISE |

### 워크플로우

```
         ┌─────────┐
         │ START   │
         └────┬────┘
              ▼
       ┌──────────────┐
       │   Analyzer   │ ← 센서 데이터 분석
       └──────┬───────┘
              ▼
       ┌──────────────┐
       │  Diagnoser   │ ← 원인 추정
       └──────┬───────┘
              ▼
       ┌──────────────┐
       │   Advisor    │ ← 조치 추천
       └──────┬───────┘
              ▼
       ┌──────────────┐
       │   Reviewer   │ ← 품질 검증
       └──────┬───────┘
              │
     ┌────────┴────────┐
     │                 │
  REVISE            APPROVE
     │                 │
     ▼                 ▼
 Diagnoser         Finalize
 (재진단)          (리포트 생성)
```

### 실행 결과

```
--- [1/4] Analyzer ---
--- [2/4] Diagnoser ---
--- [3/4] Advisor ---
--- [4/4] Reviewer ---
수정(REVISE) → Diagnoser  ← 1회 재진단
--- [2/4] Diagnoser ---
--- [3/4] Advisor ---
--- [4/4] Reviewer ---
최대 수정 횟수 → 종료

⏱️ 총 소요: 91.5초
```

**Reviewer가 부족하다고 판단 → 자동으로 재진단 → 품질 향상!**

### 생성된 리포트 (요약)

```markdown
# ConveyorGuard 진단 리포트

## 1. 데이터 분석
- NTC 0.3°C: 이상 (정상 40-60°C)
- CT1 0.0A: 이상 (정상 20-40A)
- PM2.5 0.1: 정상

## 2. 원인 추정
1. 전원 공급 이상 또는 설비 작동 정지
2. 센서 자체 고장 가능성
3. 비정상적인 환경 온도

## 3. 조치 사항
- 즉시: 전원 공급 여부 확인, 차단기 점검
- 예방: 정기 점검 주기 설정
- 점검: 센서 교정 및 교체 검토
```

---

## 5. LangGraph 사용 이유

| 이유 | 설명 |
|------|------|
| **포트폴리오 차별화** | "LLM 호출만 했어요" vs "멀티 에이전트 설계했어요" |
| **실제 현장 적용** | 예지보전은 분석→진단→조치 단계가 필요 |
| **품질 향상** | Reviewer가 부족하면 자동으로 재진단 |
| **확장성** | 에이전트 추가/수정 용이 |

### 면접 포인트

> "단순 LLM 호출이 아니라, 실제 유지보수 프로세스(분석→진단→조치→검토)를 반영한 멀티 에이전트 시스템을 설계했습니다. Reviewer 에이전트가 품질을 검증하고, 부족하면 자동으로 재진단하는 피드백 루프를 구현했습니다."

---

## 6. GPU 최적화

### 2GPU 배치 전략

```
GPU 0: Gemma-3-4B (~8GB)
GPU 1: Qwen2.5-3B (~6GB)
API  : Gemini 2.5 Flash (네트워크)
```

→ 각 모델을 별도 GPU에 배치하여 **메모리 분산 + 병렬 추론 가능**

### 메모리 관리

- 모델 한 번 로드 후 재사용 (중복 로드 방지)
- `torch.no_grad()` 로 추론 시 메모리 절약
- `device_map` 으로 GPU 지정 배치

---

## 7. 저장된 파일

| 파일 | 용도 |
|------|------|
| llm_comparison_results.csv | LLM 3종 응답 및 시간 비교 |
| langgraph_report.md | LangGraph 최종 진단 리포트 |
| llm_comparison.html | 응답 시간 비교 시각화 |

---

## 8. 파이프라인 연결

```
00_EDA
  └── 클래스 불균형 발견
        ↓
01_preprocess
  └── 윈도우 생성 + 세션 Split
        ↓
02_baseline_cnn
  └── DL Baseline → Test 92.72%
        ↓
03_ml_baseline
  └── LightGBM → Test 96.89%
        ↓
04_dl_tuning
  ├── Optuna 최적화 → Test 87.75%
  └── Ablation Study → 멀티모달 효과 입증
        ↓
05_llm_comparison (현재)
  ├── LLM 3종 비교 → Gemini 최고
  └── LangGraph → 체계적 진단 리포트
        ↓
06_ensemble
  └── ML + DL 앙상블
        ↓
07_final_comparison
  └── 다차원 비교 + 최종 결론
```

---

## 핵심 요약

### 🏆 LLM 비교 결론

| 지표 | 1위 | 비고 |
|------|-----|------|
| 속도 | Gemini 2.5 Flash | 7.99초 |
| 품질 | Gemini 2.5 Flash | 정확하고 명확한 진단 |
| 로컬 모델 | Qwen2.5-3B | Gemma보다 빠르고 안정적 |

### 🔧 LangGraph 성과

| 항목 | 결과 |
|------|------|
| 워크플로우 | Analyzer → Diagnoser → Advisor → Reviewer |
| 자동 재진단 | REVISE 시 자동 루프백 |
| 리포트 품질 | 센서 분석 + 원인 추정 + 조치 권고 포함 |
| 소요 시간 | 91.5초 |

### 💡 핵심 인사이트

1. **API vs 로컬**: Gemini(API)가 속도, 품질 모두 우수
2. **로컬 모델 한계**: T4 제약으로 소형 모델만 가능 → 품질 제한
3. **멀티 에이전트 가치**: 단순 호출 대비 체계적이고 검증된 결과 생성
4. **실제 적용 가능**: 생성된 리포트가 현장 유지보수에 바로 활용 가능

### 다음 단계

- 06_ensemble: ML + DL 결합으로 분류 성능 극대화
- 07_final_comparison: 전체 모델 다차원 비교 + 최종 결론
